{"pred": "Unanswerable. The article does not provide details on how the ground truth for fake news is established.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which is inspired from the VLAD approach for face recognition. It adds Ghost clusters along with the NetVLAD clusters, where the Ghost clusters are used to map noisy or irrelevant content into ghost clusters and are excluded during feature aggregation.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Yes, their proposed model outperforms the state of the art by 68.8% to 71.8%.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes using context tweets as an additional feature and context for neural network models. They concatenate context tweets with the corresponding labeled tweets.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at pages like FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob and Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Yes, the authors propose a baseline evaluation script and implementation.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "\nThe CNN/DailyMail dataset, the New York Times Annotated Corpus (NYT), and the XSum dataset are used for evaluation.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach captures both word similarity and entailment better than other WSD approaches employing word embeddings, as shown by the results on benchmark datasets. This is due to the use of the KL divergence energy function, which enables capturing asymmetry in entailment datasets. The approach also promotes diversity in learning senses of a word.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "They describe their ensemble method as follows: \"We started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise.\"", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The answer is \"Friends is a speech-based dataset which is annotated dialogues from the TV sitcom. It means most of the utterances are generated by the a few main characters. The personality of a character often affects the way of speaking, and therefore “who is the speaker\" might provide extra clues for emotion prediction.\"\n\nIn summary, the Friends dataset is sourced from a TV sitcom, while the EmotionPush dataset is composed of informal chats.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: how many sentence pairs does the dataset consist of?\n\nAnswer: 89,042 sentence pairs\n\nQuestion: what is the size of the vocabulary?\n\nAnswer: 82K words", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "The IMDb dataset is used for sentiment analysis.\n\nQuestion: What is the optimal combination of hyperparameters for NER?\n\nAnswer: The combination of 8 dimensions, hierarchical softmax and window size 8 performs best for NER.\n\nQuestion: How large is the GMB dataset?\n\nAnswer: The GMB dataset has 47,959 sentence samples.\n\nQuestion: What is the optimal dimension size for NER?\n\nAnswer: The optimal dimension size for NER is 300 dimensions.\n\nQuestion: What is the training time for the released Mik", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an accuracy of +1.08 F1 on DL-PS, +1.24 F1 on EC-MT, and +2.38 F1 on EC-UQ.\n\n", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Unanswerable", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Healthcare \n\n(The article does not provide an analysis of the performance across sectors.)", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "\nThey compared the RNN-based NMT model with the basic RNN-based NMT model with layer-normalization, RNN-dropout, and learning rate decay. They also compared the Transformer model with the basic Transformer model with target language reversal, residual connection, and pre-trained word2vec.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are:\n\n1. A regularization term associated with neutral features \n2. The maximum entropy of class distribution regularization term\n3. The KL divergence between reference and predicted class distribution ", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "\nThe baselines are:\n\n1) SVM with unigram, bigram, and trigram features\n\n2) SVM with average word embedding\n\n3) SVM with average transformed word embeddings\n\n4) CNN and RCNN models\n\n5) SVM and deep learning models with comment information\n\n6) UTCNN without user information\n\n7) UTCNN without topic information\n\n8) UTCNN without comments\n\nUnanswerable", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Yes, they improved the performance by 10-20% compared to the baselines, depending on the evaluation measure.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The model improves interpretability by allowing the attention heads to specialize more and with higher confidence. The adaptivity strategy relies only on gradient-based optimization, side-stepping costly per-head hyper-parameter searches.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline was a sentence-level machine translation model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "\nThe authors evaluate their bilingual models using zero-shot cross-lingual transfer tasks such as natural language inference (XNLI) and universal dependency parsing (UDP). They also evaluate the models for supervised dependency parsing. \n\n", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "Unanswerable. The article does not mention what the attention module is pretrained on.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The article obtains features from the following categories:\n\n- Lexical incongruity\n- Implicit incongruity\n- Explicit incongruity\n- Readability\n- Word count\n\nSo the answer is \"yes\".", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "Unanswerable. The article does not describe the architecture of the encoder.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is useful for taxonomic reasoning for this task, as it provides a directed graph of concepts linked by relations, which allows for naturalistic questions to be generated that test the models' ability to recognize and reason about taxonomic knowledge.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\nThe baselines were:\n\n• Jasper5x3 (a smaller Jasper model)\n• Jasper10x3 (a larger Jasper model)\n• DenseNet and DenseRNet (variants of Jasper using dense connections)\n\n", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "222,80 users\n\nQuestion: What are the three top ranked words for the \"Technology\" industry?\n\nAnswer: The top ranked words align well with what we would intuitively expect for each industry. Even though most of these words are potentially used by many users regardless of their industry in our dataset, they are still distinguished by the AFR method because of the different frequencies of these words in the text of each industry.\n\nQuestion: Is there a correlation between the gender dominance ratio and the usage of positive emotional words?\n\nAnswer: No, we did not find any statist", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "\nYes, metrics for evaluation are mentioned in the article. They include perplexity, BLEU-1/4, ROUGE-L, recipe-level coherence, recipe step entailment, and human evaluation.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for the following:\n\n• The time the patient has been experiencing the symptom\n• Activities that trigger the symptom (to occur or worsen)\n• The extent of seriousness\n• The frequency occurrence of the symptom\n• The location of symptom\n\nSo the answer is \"yes\", they create these labels on their dataset.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Unanswerable. The article does not provide any information about training the task-specific encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "\nEvaluation tasks are translation tasks. \n\n", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "Yes, there is an improvement in performance for Estonian in the NER task.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "Unanswerable\n\nExplanation: The article does not mention the background or experience of the researchers. The researchers are described as having diverse disciplinary backgrounds and research practices, but their specific backgrounds are not mentioned.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes. The paper proposes an unsupervised approach to spam detection based on the LDA model and topic-based features.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "Yes. The article mentions that the Nguni languages are similar to each other and the Sotho languages are also similar.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the 6-layers and 9-layers sMBR models. ", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The Wikipedia dataset consists of around 5K FA, 28K GA, 212K B, 53K C, 2.6M Start, and 3.2M Stub articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled through a collection of samples of about 100 sentences taken from the test set results. A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A randomized selection of the translation results was taken for comparison.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes\n\nExplanation: The article states that they test their framework performance on an English-German translation task, as well as a zero-resourced translation task where there is no parallel data between the languages.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by quantifying the efficiency of a communication scheme (retention rate of tokens) and the accuracy of a scheme (reconstruction loss). ", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "\nYes, the article mentions that precision, recall and F-measure are used as evaluation metrics for sentence classification tasks. Precision and recall are computed as follows: \n\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nF-Measure = 2 \\* Precision \\* Recall / (Precision + Recall)", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "Unanswerable. The article does not provide any information about the source and target domains.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "The article compares their PRU model with the state-of-the-art LSTM model (AWD-LSTM) and other recent RNN models (RAN, QRNN, NAS). \n\nSo the answer is: Yes, they compare their PRU model with LSTM and other RNN models.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "The Block Zoo provides common layers like RNN, CNN, QRNN, Transformer, and attention mechanisms.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. They also used the Carnegie Mellon Pronouncing Dictionary, though it is a monolingual English resource and is unsuitable for their multilingual task.\n\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were BERT, RoBERTa, and XLNet.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English, Spanish, Finnish", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "Unanswerable. The article only tests their method on predicting hashtags for social media posts.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Unanswerable based on the article. The article does not mention anything about using pretrained word embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They generate maps that reflect psycholinguistic and semantic word categories. They use distribution of words in categories like \"Positive Feelings\" and \"Money\" to generate maps.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "\nThe machine learning models aim to identify the following argument components:\n\n• Claim\n• Premise\n• Backing\n• Rebuttal\n• Refutation\n\n", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Unanswerable. The article does not mention any specific length threshold for aligned n-grams.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "Unanswerable\n\nThe article does not mention the size of the Twitter dataset. It only mentions that they downloaded 1,873 Twitter conversation threads, roughly 14,000 tweets, from a publicly available resource. The article does not provide any information about the size of the Twitter dataset. Therefore, the question cannot be answered based on the information provided.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes, the 12 languages covered are English, French, Spanish, German, Polish, Finnish, Welsh, Russian, Yue Chinese, Korean, Kiswahili, Estonian, Czech, Arabic, and Hebrew.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia conversations dataset and Reddit ChangeMyView dataset\n\nUnanswerable: The article does not provide information on what datasets the model is applied to.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes. The paper mentions that they used Freeling BIBREF8 library to provide part-of-speech tags, named entity recognition, dependency parsing, semantic role labeling, and lexicon matching.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by comparing the translations produced by professional translators against the human translations, using metrics such as BLEU score, perplexity, ratio of English characters, and similarity scores.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use dual RNNs to encode audio and text data independently, then combine the information from these sources using a feed-forward neural model to predict the emotion class.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Yes, their model improved by 2.1 BLEU, 1.7 FKGL and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The article does not provide information on how many humans evaluated the results. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Based on the article, tweets are considered to go viral if they are retweeted more than 100 times. \n\n", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "Unanswerable. The article does not provide information on which basic neural architecture (like CNN, Logistic Regression, etc.) performs best by itself.", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing. \n\n", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The article uses Logistic Regression trained on the clinical-RQE dataset to recognize entailed questions and rank them. ", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "\nYes, the Social Honeypot dataset is a benchmark dataset used in the paper to evaluate the proposed features. Its quality is considered high as it contains 19,276 legitimate users and 2,223 spammers.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder followed by an LSTM decoder. ", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes. They only report results on English data.\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "Yes, the ensemble of Logistic Regression, CNN and BERT was ranked 3rd among the author's submissions, with an F1 score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a weak baseline without using any monolingual data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "Their highest recall score was 0.7033.\n\nNote: The article does not provide the actual number, only the phrase \"the highest recall score of ‘0.7033’\" is mentioned.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores integrating semantic similarity measures into second-order co-occurrence vectors. It does not directly compare or explore other embedding techniques like word embeddings. Therefore, the answer is \"unanswerable\".", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use a system called CFILT-preorder which contains generic rules and Hindi-tuned rules. The generic rules apply to all Indian languages while the Hindi-tuned rules improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes. The paper mentions that a ranked list of interacting proteins that are extracted from biomedical literature but are not present in existing databases can allow researchers to make informed decisions about which protein/gene to study further. This suggests that the paper does explore extraction from electronic health records.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Experts with legal training were used for annotation.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\nThe image embedding model is CNN-RNN. The language style transfer model is a sequence-to-sequence model with parallel text corpus for text style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "Unanswerable\n\nThe article does not compare the performance of the RNN layer and transformer layer directly. It only discusses the advantages and disadvantages of the two techniques - RoBERT and ToBERT - as extensions to BERT for long documents.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes. ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation for relation classification called \"extended middle context\". They split the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. They force the network to pay special attention to the middle context by using two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n\nTherefore, the answer is \"proposed\".", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\nYes, the dataset contains three types of entities: Person, Location and Organization.", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Unanswerable\n\nThe article does not provide information on how much higher quality the resulting annotated data is.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The imbalance is significant, with 65% of speakers being men, who speak 75% of the time.\n\nQuestion: Is there a correlation between gender and speaker's role?\n\nAnswer: Yes, there is a correlation between gender and speaker's role, with Anchors speaking more than Punctual speakers.\n\nQuestion: Does the ASR system exhibit a gender bias?\n\nAnswer: Yes, the ASR system exhibits a gender bias, with women having a higher WER than men.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "French-German.\n\nExplanation: According to the article, \"Our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30.\"", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The paper compares the proposed model to various strong baseline models like BIBREF18, BIBREF17, BIBREF11, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF29, BIBREF30, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The article does not mention any specific classifiers. It discusses logistic regression and multilayer perceptron models as the target models for event detection. \n\nTherefore, the answer is: Unanswerable.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26, and spaCy.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "existing approaches include using bag-of-words representations of geographic locations, using word embeddings based on geographic coordinates, combining word embeddings with geographic coordinates, and using knowledge graphs to model geographic locations.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention in the contextual encoding layer.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "\nThey used the CSAT dataset, 20 newsgroups dataset, and Fisher dataset for evaluation.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "The IMDb movie review dataset is used for sentiment classification experiments.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the neural projector satisfies the following two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema is described in detail in Appendix.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The dataset sizes are:\n\n- WikiSmall: 89,042 sentence pairs\n- WikiLarge: 296,402 sentence pairs", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "\nThe baselines are:\n\n- Vanilla ST baseline\n- Pre-training baselines\n- Multi-task baselines\n- Many-to-many+pre-training\n- Triangle+pre-train", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.\n\nPhrase: The paper studies English.\n\nUnanswerable: The paper does not mention any natural language besides English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiments are:\n\n- A linear SVM trained on word unigrams\n- A bidirectional Long Short-Term-Memory (BiLSTM) model\n- A Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 ", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the article does not mention any measures for evaluating the usefulness of the answers.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "unanswerable\n\nThe article does not mention any pretrained word embeddings being used. The feature extractors mentioned are lexicon based, such as BING Liu and NRC Affect Intensity, and not word embeddings.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "They showed that their personalized models outperformed the baseline model in perplexity, BLEU-1, ROUGE-L, and recipe-level coherence metrics. They also found that the Prior Name model produced the most diverse recipes and the highest likelihood of recipes conditioned on a particular user profile.\n\n", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is a harmonic mean of irony reward and sentiment reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model has limitations when generating Shakespearean prose for paintings that have a low content score. They note that this happens when the style transfer dataset does not have similar words in the training set of sentences. \n\nUnanswerable: The article does not provide information on the authors' motivations or personal experiences.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. ", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution results are presented in Figures 24-30 and Tables 23 and 3.\n\nThe answer is: Yes.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is constructed by crowdsourcing the segmentation of 1,268 randomly selected tweets from the Stanford Sentiment Analysis Dataset. The authors also use their own additional corrections and adapt the Word Breaker model to generate initial candidate segmentations.\n\n", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "Unanswerable\n\nThe article does not mention any information about accents present in the corpus. Therefore, it is unanswerable to determine what accents are present.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent most of the variability of the data in each class of the Reuters-8 database.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The article does not explicitly mention what baseline model is used. Therefore, the answer is \"unanswerable\".", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Unanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "Unanswerable\n\nExplanation: The article does not mention the size or details of the Augmented LibriSpeech dataset.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "They used the fine-grained sentiment classification dataset from SemEval-2016.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Unanswerable", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Unanswerable\n\nThe article mentions that while the automatically constructed datasets are generated at virtually no cost and with little manual annotation, it is harder to validate the quality of such data at scale. The authors also note that while human expert sources are used to construct the datasets, human-curated data is fallible and error-prone. The article does not provide any information on how the quality of the datasets is controlled or validated. Therefore, the answer is \"unanswerable\".", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Unanswerable based on the article. The article does not mention anything about the domain of the images used for evaluation.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "According to the article, their best model achieved a micro-averaged f-score of 0.368 on the development set.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The INLINEFORM0 tagging scheme is employed.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Unanswerable\n\nExplanation: The article does not mention Arabic as one of the 11 languages in CoVoST. It only mentions 11 languages in the introduction: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The article does not define robustness of a model. Therefore, the answer is unanswerable based on the information provided in the article.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "The article evaluates the following sentence embeddings methods:\n\n- Average GloVe embeddings\n- InferSent \n- Universal Sentence Encoder\n- Polyencoders\n- Skip-Thought \n- Sentence-BERT (SBERT)\n\nSo the answer is: Evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "\n\nFor English datasets, proposed method improves F1 score by +0.29.\n\nFor Chinese datasets, proposed method improves F1 score by +0.97 and +2.36.\n\n", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": " They test their conflict method on two tasks: Quora duplicate question pair detection and ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Unanswerable\n\nThe article does not mention any specific baselines or comparisons against other models. The only comparisons are against \"latent tree-based models\" and \"structurally pre-trained models\" in the quantitative analysis section.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the hierarchical matching between questions and KB relations. The hierarchical matching approach allows the model to match questions to different levels of relation representations, which helps improve the relation detection and KBQA end-task performance.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "\nThe baseline models are:\n\n- A name-based Nearest-Neighbor model (NN)\n- A simple Encoder-Decoder baseline with ingredient attention (Enc-Dec)\n\nSo the answer is \"yes\".", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "\nLinguistic bias can be found through the distribution of terms used to describe entities in a particular category. Unwarranted inferences are identified through analyzing the descriptions for what goes beyond what the visual data alone can tell us.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "French, Spanish, Italian, Portuguese, and some Semitic languages like Arabic and Hebrew. ", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with stacked LSTMs, cell-aware stacked LSTMs, bidirectional cell-aware stacked LSTMs, and a baseline stacked LSTM model. ", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they report results on both English and multilingual data.\n\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with Sumy algorithms and an ILP-based summarization algorithm.\n\n", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was models proposed by BIBREF0 .", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "\nUnanswerable. The article does not discuss or analyze the impact of any specific component on the overall performance of MPAD.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, consisting of subparts of the DTA corpus which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " Yes. According to the article, they collect and curate 635 hours of audio data for 7 Indian languages: Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model achieves competitive performance on target language reading comprehension. \n\nQuestion: Does the model rely on language-independent strategies?\n\nAnswer: Yes, the model seems to rely on language-independent strategies.\n\nQuestion: How does the model handle code-switching?\n\nAnswer: The model can handle code-switching data, but its performance drops.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "Yes, the proposed model ALOHA achieves a significant improvement in performance compared to baselines. The results show that ALOHA achieves a noticeable boost in accuracy for all five evaluation characters, demonstrating that the model is robust and stable and able to recover the dialogue styles of fictional characters regardless of the character's identity, genre of show, and context of dialogue.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article does not provide an explicit answer to this question. The improvements gained from ARAM are only compared to other baseline models, but the magnitude of the improvements is not quantified. Therefore, the answer is \"unanswerable\".", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining tweets that were misclassified as offensive or neither. They also discuss how some of the biases in the data may have contributed to the model's misclassifications.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, several baselines were tested and compared to the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset contains 64,000 sentences from the year 2015-2016. \n\n", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed Dice loss (DL) introduces performance boost for both BERT and XLNet. Using DL introduces performance improvement by +0.58 for MRPC and +0.73 for QQP. \n\n", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Unanswerable. The article does not mention any datasets used.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The article does not mention any specific data presented to subjects. It only discusses the EEG data collected from subjects during imagined speech tasks. \n\nSo answer: Unanswerable", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen+RL+ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "\nNeural Network based Models:\n\n- Convolutional Neural Networks (CNN)\n- Recurrent Neural Networks (RNN)\n- HybridCNN\n\nTraditional Machine Learning Models:\n\n- Naïve Bayes (NB)\n- Logistic Regression (LR)\n- Support Vector Machine (SVM)\n- Random Forests (RF)\n- Gradient Boosted Trees (GBT)", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The article does not provide information about the specific language model architectures used. The pre-trained language models are estimated on the German and English newscrawl data. The bi-directional model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2. The uni-directional model contains INLINEFORM0", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "Weight dynamically adjusted by associating each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Yes, the proposed strategies show that KG-A2C-chained and KG-A2C-Explore are able to pass the bottleneck of entering the cellar and lighting the lamp in Zork1, while A2C-Explore fails. ", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a Bayesian model of garg2012unsupervised as the monolingual model, and additional crosslingual latent variables to incorporate soft role agreement between aligned constituents.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The article mentions that annotations for non-standard pronunciation include aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.\n\nSo the answer is: Yes.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar architecture refers to a word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. ", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "Yes. According to the results in Table 2 and Figure 28, NCEL achieves the best performance in most cases and consistently outperforms baseline methods on various datasets, demonstrating its effectiveness.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nThe article mentions that the data is de-identified by removing identifying information and using digital zeros and [de-identified] tags.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Unanswerable", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "\nUnanswerable. The article does not mention obtaining any annotated clinical notes. The annotated sentences were extracted from clinical notes, but there is no mention of obtaining the annotations themselves.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "MS.\nBMS.\nFMS.MS.MS.\n\nMS.MS.\nM.\nFMS.MS.\nBMS.MS.MS.\nMS.\nBMS.MS.MS.MS.MS.\nMS.\nMS.\nMS.\nBMS.MS.MS.\nMS.MS.\nMS.\nM.MS.\nMS.MS.\nM.MS.MS.\nMS.MS.\nMS.MS.MS.\nMS.MS.MS.\nMS.MS.MS.\nMS.MS.MS.\n", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "Unanswerable\n\nThe article does not mention any specific dataset used to train the models. The models are trained on various datasets like PPDB, Twitter, and distant-supervised tweets.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 annotations for 9,30 tweets. Each tweet is annotated as no evidence of depression (e.g., \"Citizens fear an economic depression\") or evidence of depression (e.g., \"depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., \"feeling down in the dumps\").", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\nYes. They evaluated GreenBioBERT on eight biomedical NER tasks. ", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated using the machine translation platform Apertium.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "The paper does not mention the specific model they used for their system. The authors only discuss the feature selection methods and content-based classifiers. Therefore, the answer is unanswerable based on the information in the article.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Yes, the article mentions a baseline system for the sentence-level classification task that uses a simple logistic regression classifier based on the length of the sentence.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The paper compares their joint pun detection and location approach with the following baselines:\n\n- A CRF model based on POS tags, n-grams, label transitions, word suffixes, and relative position to the end of the text \n- A CRF model based on the INLINEFORM0 tagging scheme\n- A pipeline method where the classifier for pun detection is regarded as perfect\n\nSo the answer is: \n\nYes", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Unanswerable. The article does not discuss how political bias is included in the model.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from 1.7K bilingual ancient-modern Chinese articles collected from the internet.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Unanswerable", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable. The article does not mention any Chinese datasets.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has 3 convolutional layers and 1 average pooling layer. \n\n\"3 convolutional layers and 1 average pooling layer\"", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The paper uses the Natura 2000 dataset for predicting species distribution, the ScenicOrNot dataset for predicting scenicness, and various numerical and categorical features for the regression tasks.\n\nSo, the answer is: yes.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses the NUBes-PHI and MEDDOCAN datasets.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unanswerable\n\nThe article does not mention any traditional linguistic features used. It only discusses cognitive features derived from eye-tracking data.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "Metrics include coverage of query data instances, average MCC and average +ve F1 score.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "No. They do not employ an indexing-based method to create a sample of a QA Wikipedia dataset.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "The article mentions that the targets of the stance detection are two popular sports clubs in Turkey, specifically Galatasaray and Fenerbahçe.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Experiments are conducted on the transformation from non-ironic sentences to ironic sentences and the reverse transformation.\n\n", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "The Gaussian-masked directional multi-head attention is a variant of scaled dot-product attention that aims to pay attention to adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention. The Gaussian weight matrix $G$ represents the localness relationship between each two characters. The Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by queries and keys is used to combine the Gaussian weights. The self-attention sublayers needs to use a triangular matrix mask to focus on different weights.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Unanswerable. The article only considers Facebook status updates.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the features extracted directly from the CNN network, without using any pre-trained models.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "\n\nYes, the paper varied the number of clusters (INLINEFORM1) and the dimensionality of the word embeddings (INLINEFORM0).", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "Second place (EI-Reg) and second place (EI-Oc)", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.5 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. \n\nSo the answer is: 167,739 words", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "No, the article does not mention anything about converting cloze-style questions to naturally-looking questions. The cloze construction process is described as generating clozes by replacing placeholder words in the introduction with actual words. ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization. \n\nExplanation: The article mentions \"text categorization\" as one example of a natural language processing task where they leverage prior knowledge.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The BERT-QC model is compared to a combination of rule-based and machine learning methods, as well as CNN and BERT variants.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The article mentions that the existing ELMoForManyLangs models were trained on a smaller dataset of 20 million tokens. The new versions of ELMo for the seven languages were trained on larger corpora, ranging from 270 million tokens for Latvian to 800 million tokens for Finnish.\n\n", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 120,000 sentences. ", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare their proposed s2sL model to MLP model. They also compare their s2sL model to other state-of-the-art methods like Eusboost and MWMOTE for imbalanced data classification tasks.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Unanswerable. The article only discusses experiments on the Wall Street Journal portion of the Penn Treebank corpus. They do not mention evaluating on any other language datasets.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "According to the article, their highest MRR score was 0.6103 in one of the test batches for Factoid Question Answering task. \n\nSo the answer is \"0.6103\" or \"yes\".", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "Wall Street Journal (WSJ) portion of the Penn Treebank. \n\n", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors cite the survey they conducted among engineers to support their claim. According to the survey, \"more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on.\" This suggests that many engineers use a combination of components from different sources, which can be time-consuming and complex.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve state of the art on the SimpleQuestions and WebQSP benchmarks.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
