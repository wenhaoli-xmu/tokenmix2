{"pred": "The ground truth for fake news is established by a single expert who manually annotated the dataset of viral tweets. The expert categorized each tweet as containing fake news or not containing it based on the content of the tweet and the meta-data.\n\nThe expert used the categorization provided by Rubin et al. BIBREF7 to identify fake news. The categories were:\n\n1. Serious fabrication\n2. Large-scale hoaxes\n3. Jokes taken at face value\n4. Slanted reporting of real facts and stories where the truth is contentious.\n\nThe", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is a pooling strategy proposed by W. Xie et al. [9] for language identification. It uses Ghost clusters to map noisy or irrelevant content into ghost clusters and is not included during feature aggregation. The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage.\n\nThe GhostVLAD pooling approach is used in the NetVLAD layer of the proposed pooling strategy for language identification. The NetVLAD layer is kept on top of the feature extraction layer of ResNet, as", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The article states that their proposed model outperforms the state-of-the-art results by a substantial margin. The WAP value of their model is 0.718, which is 1.47% higher than the best existing result of 0.710. The MDRE model achieves the state-of-the-art performance with a WAP value of 0.718, which is 1.47% higher than the best existing result of 0.710.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes several additional features and contexts to improve the performance of neural network models for abusive language detection. These include:\n\n1. Using context tweets to provide additional information about the target tweet. This is done by concatenating the output of max-pooled layers from word-level and character-level CNN, and feeding this vector to a fully-connected layer in order to predict the output.\n\n2. Applying a self-matching attention mechanism on RNN baseline models to better understand the data by retrieving text sequences twice.\n\n3. Implementing a Lat", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "The article does not provide a specific list of Facebook pages that were looked at. However, it mentions that they used Facebook pages that had been released as development data for SemEval's 2007 Task 14. This suggests that they may have looked at some of the Facebook pages that were released as part of the development data. However, it is not clear which specific pages they looked at.\n\nQuestion: How did they select the Facebook pages to use?\n\nAnswer: The article does not provide specific details on how they selected the Facebook pages to use. However, it mentions that they used pages that had", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The hashtag and SemEval datasets contain only English data. The hashtag segmentation model uses a language model trained on 476 million tweets in English. The SemEval dataset contains tweets in English.\n\nQuestion: How does the hashtag segmentation model work?\n\nAnswer: The hashtag segmentation model uses a language model trained on 476 million tweets in English. It then applies a hashtag segmenter that uses a set of word-shape rules to split hashtags into their constituent words. The model then predicts whether each word is part", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The proposed task of concept map annotation is evaluated using a corpus of 30 topic clusters of 40 documents each. The corpus contains 15 times more documents than typical DUC corpora and is 15 times larger in terms of cluster size. The authors propose a new evaluation scheme that includes a corpus-level evaluation and a fine-grained ranking of the most important propositions.\n\nThe authors also provide a baseline method and evaluation scripts to facilitate future research on this variant of summarization.\n\nHope this helps! Let me know if you have any other questions.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The datasets used for evaluation are:\n\n1. CNN/DailyMail: This is a dataset of 226,711 news articles with summaries. The summaries are 110,540 abstractive summaries and 115,171 extractive summaries.\n\n2. NYT: This is a dataset of 488,418 news articles with summaries. The summaries are 1,093,610 abstractive summaries and 1,093,610 extractive summaries.\n\n3", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The article proposes a KL divergence based energy function for learning multi-sense word embedding distributions. It argues that polysemous words should be represented by a mixture of Gaussian distributions to capture textual entailment. The proposed approach combines the KL divergence approximation methods of BIBREF12 with a variational approximation method to obtain stricter bounds on KL between gaussian mixtures. The authors claim that their approach achieves better performance compared to other WSD approaches employing word embeddings.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by combining the predictions of multiple models. The models are selected based on their performance on the validation set. The ensemble is formed by simply averaging the predictions of the constituent models.\n\nQuestion: What is the difference between the psr model and the other models?\n\nAnswer: The psr model is a simple model that simply sums the attention across all occurrences of each unique word in the question. It does not use any complex architectures or techniques. The other models use more complex architectures and techniques, such as using attention over the context document or using two-way context-question attention.\n", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The article mentions that the Friends dataset is collected from the Friends TV show, and the EmotionPush dataset is collected from Twitter. The article does not provide any specific details about the sources of the datasets.\n\nQuestion: What are the limitations of the proposed approach?\n\nAnswer: The article mentions that the proposed approach has some limitations, including the limitation of the BERT input format, the limitation of the number of input sentences, and the limitation of the personality embedding development. The article also mentions that the proposed approach is not able to tackle the problem of highly unbalanced emotion labels.\n\nQuestion:", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "The paper focuses on simplifying text in English. It proposes using simplified corpora during training of neural text simplification systems.\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: The main contribution is the proposal to use simplified corpora during training of neural text simplification systems. The paper shows that this can improve the results of the systems.\n\nQuestion: what are the limitations of the proposed method?\n\nAnswer: The paper mentions some limitations, such as the need for more parallel and simplified corpora, and the impact of domain. It also notes that the results may vary depending on the", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "The work uses the IMDb dataset for sentiment analysis. The IMDb dataset is a dataset of movie reviews by BIBREF13.\n\nQuestion: What is the objective of the work?\n\nAnswer: The objective of the work is to determine the optimal combinations of word2vec hyper-parameters for intrinsic and extrinsic NLP tasks. The work aims to answer the question: what is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nQuestion: What are the key results of the work?\n\nAnswer: The key results of the work are:\n\n1", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an accuracy of 91.19% on the EC-UQ dataset. This is the highest accuracy among all systems tested. The proposed system also achieves an accuracy of 89.12% on the EC-MT dataset, which is the second highest. The proposed system achieves an accuracy of 86.22% on the DL-PS dataset, which is the third highest.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they experimented with the new dataset. They describe the materials and experiment design in detail, and conduct an extensive validation to ensure the quality of the data. They also describe the applications and re-use potentials of the data.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: The purpose of the experiment is to construct a new corpus of eye-tracking and EEG recordings during natural reading and annotation, and to compare the two reading paradigms. They aim to provide a dataset that allows for the comparison between normal reading and reading during annotation, and to improve the quality of annotations for", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The article mentions that the datasets used are the ones provided by the system developers. It does not provide any specific details about the datasets.\n\nQuestion: What is the purpose of the system?\n\nAnswer: The purpose of the system is to provide a tool for integration tests for the CognIA system. It allows developers to simulate user interactions with the system and check if the system is behaving as expected.\n\nQuestion: What are the limitations of the system?\n\nAnswer: The article mentions some limitations of the system, such as the need for manual intervention to add new classes and samples when the system is stable enough", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The article does not provide a clear answer to this question. The best performing sector is not explicitly stated. However, the article does mention that the HealthCare and Energy sectors had the highest accuracy scores, but it does not provide a comparison of the performance across all sectors.\n\nThe article also mentions that the performance of the models varied across sectors, with some sectors performing better than others. This suggests that the best performing sector may depend on the specific model architecture and hyperparameters used.\n\nIn summary, the article does not provide a definitive answer to this question. The best performing sector may depend on the specific", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The article compares the following NMT models:\n\n1. RNN-based NMT (RNN-BIBREF19)\n\n2. Transformer-based NMT (Transformer-BIBREF20)\n\n3. SMT (SMT-BIBREF21)\n\n4. RNN-based NMT with target language reversal (RNN-BIBREF22)\n\n5. RNN-based NMT with residual connection (RNN-BIBREF23)\n\n6. RNN-based NMT with word2vec", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are:\n\n1. Incorporating neutral features. This term is simply the KL divergence between the predicted and reference class distributions. It does not require any knowledge about the true class distribution.\n\n2. Maximum entropy. This term is the KL divergence between the predicted and reference class distributions, but it requires knowledge about the true class distribution.\n\n3. Maximum entropy with the reference class distribution. This term is the KL divergence between the predicted and reference class distributions, but it also requires knowledge about the true class distribution.\n\nIn summary, the three regular", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are:\n\n1. SVM with unigram, bigram, and POS-dependencies\n\n2. SVM with average word embedding\n\n3. SVM with average transformed word embedding\n\n4. Two mature deep learning models on text classification, CNN and RCNN\n\n5. UTCNN without user information\n\n6. UTCNN without topic information\n\n7. UTCNN without comments\n\n8. UTCNN without the LDA model\n\n9. UTCNN without supportive posts\n\n10. UTCNN without unsupportive posts\n\n11. UTCNN", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The article states that the authors \"showed that by jointly learning the tasks of ternary and fine-grained classification with a multitask learning model, one can greatly improve the performance on the second.\" However, the article does not provide a specific quantitative improvement. The authors mention that the biLSTM network with multitask learning achieves the best performance, but they do not provide a specific percentage or absolute improvement.\n\nQuestion: What are the limitations of the approach?\n\nAnswer: The article mentions several limitations of the approach, including:\n\n1. The approach relies on the foundations", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The authors argue that their adaptively sparse Transformer can dynamically select a sparsity pattern that finds relevant words regardless of their position (e.g., Figure FIGREF52). This allows the model to specialize more and with higher confidence in its predictions. In contrast, softmax transformers are fixed sparsity patterns that do not adapt to different positions.\n\nThe authors also find that the adaptively sparse Transformer can dynamically spread weight sparsely over 2-3 neighboring tokens, when the tokens are part of the same BPE cluster or cluster of hyphenated words. This type of heads in the", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline was a context-agnostic model that only considered the input sentence and did not take into account the context of other sentences in the document.\n\nQuestion: what was the DocRepair model?\n\nAnswer: The DocRepair model was a monolingual model that took into account the context of other sentences in the document. It was able to correct inconsistencies between sentence-level translations.\n\nQuestion: what was the improvement in BLEU score?\n\nAnswer: The BLEU score improved by 0.7 BLEU points on average.\n\nQuestion: what was the", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article mentions that the metrics used for evaluation are:\n\n1. Cross-lingual Natural Language Inference (XNLI)\n\n2. Dependency parsing\n\n3. Zero-shot Experiments\n\n4. Universal Dependencies v2.4\n\n5. XNLI test accuracy\n\n6. LAS scores\n\n7. LAS scores for zero-shot tasks\n\n8. LAS scores for supervised tasks\n\n9. LAS scores for dependency parsing\n\n10. LAS scores for zero-shot and supervised tasks\n\n11. LAS scores for", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on the MT data. The paper states that the attention module is pretrained on the MT data using the Transformer model. The paper also mentions that the attention module is pretrained on the MT data using the seq2seq model.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The article mentions that the authors obtain \"stylistic features\" from the eye-movement data. However, it does not provide any specific details about what these features are. The authors mention that they use \"readability features\" and \"word count features\" but do not provide any details about what these features are.\n\nQuestion: How do the authors combine the linguistic features with the eye-movement features?\n\nAnswer: The authors combine the linguistic features with the eye-movement features using a \"multi-instance learning\" approach. This means that they use the linguistic features to create \"bags\"", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM encoder with an LSTM decoder.\n\nQuestion: What is the purpose of the auxiliary objective of MSD prediction?\n\nAnswer: The auxiliary objective of MSD prediction is to increase the morpho-syntactic awareness of the encoder and to regularise the learning process.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\n\nAnswer: Encoding the full context with an LSTM highly increases the variance of the observed results.\n\nQuestion: What is the effect of adding the auxiliary objective", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "WordNet is not useful for taxonomic reasoning for this task. The article states that WordNet is not designed for this task and that the authors had to manually curate the data to create the taxonomic clusters. The authors also mention that the clusters are based on the concept/synset \"oppose.v.06\" and involve 4 inferences and a total of 24 questions (i.e., inferences with perturbations). The article does not provide any evidence that WordNet is useful for taxonomic reasoning for this task.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines used in the paper were:\n\n1. Standard convolutional neural networks (CNNs) with batch normalization and ReLU.\n\n2. Dense Residual (DR) networks, which are CNNs with residual connections.\n\n3. Dense Residual (DR) networks with batch normalization and ReLU.\n\n4. Dense Residual (DR) networks with batch normalization and ReLU, weight normalization, and dropout.\n\n5. Dense Residual (DR) networks with batch normalization and ReLU,", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "The article does not provide a specific number of users that the company looks at. However, it does mention that the company uses \"all the features\" from the users' posts, which suggests that the company looks at all the users' posts.\n\nQuestion: What is the gender distribution of the users?\n\nAnswer: The article does not provide a specific gender distribution of the users. However, it does mention that the company uses \"all the features\" from the users' posts, which suggests that the company looks at all the users' posts.\n\nQuestion: What is the industry distribution of the users?\n\nAnswer:", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "The metrics used for evaluation are:\n\n1. BLEU-1 and BLEU-4: These are metrics that measure the similarity between generated and reference recipes in terms of n-gram matching. They are used to evaluate the quality of the generated recipes.\n\n2. MRR: This is a metric that measures the recipe-level coherence of the generated recipes. It is calculated by averaging the recipe-level coherence scores of all generated recipes.\n\n3. UMA: This is a metric that measures the personalization of the generated recipes. It is", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels created on the dataset are:\n\n1. \"completed symptoms\" - symptoms that have been discussed and completed\n\n2. \"to-do symptoms\" - symptoms that have not been discussed and are still to be discussed\n\n3. \"completed attributes\" - attributes that have been discussed and completed\n\n4. \"to-do attributes\" - attributes that have not been discussed and are still to be discussed\n\n5. \"unanswerable\" - symptoms that cannot be answered based on the information in the article\n\n6. \"yes\" - symptoms that have been discussed", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The article does not provide a specific answer to this question. However, it does mention that the task-specific encoder is trained on data sampled from the most difficult instances. This suggests that the task-specific encoder may require more data than the universal encoder. The article also mentions that the task-specific encoder is trained on data sampled from the most difficult instances, which suggests that the task-specific encoder may require more data than the universal encoder.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The article mentions that the authors evaluate their proposed adaptively sparse Transformer on the WMT14 English-German translation task. They use the BLEU metric to evaluate the model's performance.\n\nQuestion: What is the role of the $\\alpha $ parameter in the adaptively sparse Transformer?\n\nAnswer: The $\\alpha $ parameter is used to dynamically adjust the continuous attention weights assigned to each token in the cluster. The authors propose that this parameter can be used to dynamically adapt the model's sparsity patterns and improve performance.\n\nQuestion: How does the adaptively sparse Transformer compare to fixed", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The article states that the ELMo embeddings for Estonian show the largest improvement over fastText embeddings for the NER task. The ELMo embeddings for Estonian produce substantially better results compared to the non-contextual fastText baseline.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "The authors have a background in computational linguistics, natural language processing, and social science research. They have used text analysis techniques to study hate speech in social media and have also applied these techniques to other social science research topics.\n\nQuestion: What is the main claim of the article?\n\nAnswer: The main claim of the article is that text analysis techniques can be used to study social and cultural phenomena beyond traditional social science research topics. The authors argue that text analysis can provide new insights into social and cultural phenomena by allowing us to explore subtle patterns and explore radical new questions.\n\nQuestion: What are the limitations", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes, this paper introduces an unsupervised approach to spam detection using Latent Dirichlet Allocation (LDA). LDA is an unsupervised topic model that can be used to extract topic-based features for spam detection. The paper proposes using LDA to extract two topic-based features: Global Outlier Standard Score (GOSS) and Local Outlier Standard Score (LOSS). These features are based on the topic distributions of spammers and legitimate users. The paper shows that these features can effectively detect spammers who post seemingly legitimate tweets.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The article mentions that the proposed algorithm is dependent on the support of the lexicon. The lexicon is a dictionary of words and their meanings. The article does not provide a list of similar languages, but it does mention that the proposed algorithm is applied to short texts, informal styles and similar languages. Therefore, it is likely that the proposed algorithm is dependent on the support of a lexicon that includes words and their meanings for similar languages.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The article compares the performance of the proposed Shenma model with the following LSTM models:\n\n1. 2-layers regular-trained Amap model\n\n2. 2-layers regular-trained Shenma model\n\n3. 2-layers regular-trained CE model\n\n4. 2-layers regular-trained BMUF model\n\n5. 2-layers regular-trained sMBR model\n\n6. 2-layers regular-trained sMBR model with distillation\n\n7. 2-layers regular-", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The article mentions that the dataset consists of 29,794 articles from Wikipedia and arXiv. The dataset is split into training, development, and test splits based on a ratio of 8:1:1. The article does not provide the exact number of articles in each split.\n\nQuestion: What are the criteria for the quality classes?\n\nAnswer: The article mentions that the quality classes are based on the Wikipedia grading scheme, which uses criteria such as \"Featured Article\" (FA) for the highest quality, \"Start\" (Start) for articles that are \"usually of a high", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by the authors of the paper. They used a group of 50 native people who were well-versed in both English and Tamil languages for the evaluation. The judgements were based on the BLEU scores, adequacy, fluency and relative ranking values. The judges were asked to rate the models based on the BLEU scores and the other metrics.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They report the performance of their systems on two German INLINEFORM0 French test sets. They compare their systems to a direct system, which is an NMT trained on German INLINEFORM1 French data, and to a pivot system, which is a many-to-many multilingual NMT.\n\nQuestion: What are the limitations of their approach?\n\nAnswer: The limitations of their approach include:\n\n1. The target forcing mechanism is moderately primitive. The target forcing symbol is the same for all", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated in this human-machine communication game by measuring the accuracy and efficiency of the schemes that the models generate. The accuracy is measured by how well the generated scheme can reconstruct the target sentence, while the efficiency is measured by how many tokens are retained in the generated scheme. The authors use the retention rate of tokens as a proxy for efficiency.\n\nThe authors also provide additional experiments to analyze the tradeoff between accuracy and efficiency, and to evaluate the stability of the schemes. They show that the constrained objective of minimizing the expected cost while keeping the reconstruction error below a threshold leads to more stable schemes compared", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The article mentions several evaluation metrics that are looked at for classification tasks:\n\n1. Precision and recall: These are used to evaluate the performance of the classifiers on the test set. Precision is the proportion of true positives among the predicted positive examples, while recall is the proportion of true positives among all the examples in the test set.\n\n2. F-measure: This is a weighted average of precision and recall, where the weighting is based on the proportion of positive examples in the test set.\n\n3. ROUGE: This is a standard metric for evaluating the quality of", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain where the labeled examples are available. The target domain is the domain where the unlabeled examples are available.\n\nQuestion: What is the problem the authors are trying to solve?\n\nAnswer: The authors are trying to solve the problem of domain adaptation in unsupervised domain adaptation. They want to find a way to effectively transfer knowledge from the source domain to the target domain without using labeled examples from the target domain.\n\nQuestion: What is the proposed method called?\n\nAnswer: The proposed method is called DAS (Domain Adaptive Semi-supervised).\n\n", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "The article compares the performance of the PRU with several previous RNN models, including LSTMs, BLSTMs, and PRUs with different architectures. The authors note that the PRU achieves the best performance among the compared models.\n\nQuestion: what is the difference between the PRU and the LSTM?\n\nAnswer: The PRU differs from the LSTM in the following ways:\n\n1. The PRU uses a pyramidal transformation to learn higher dimensional representations with more generalizability. The LSTM uses a linear transformation to learn lower dimensional representations.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes the following modules:\n\n1. Embedding Layer: Word/character embedding and extra handcrafted feature.\n\n2. Sequence Labeling: Predict each token in a sequence into predefined types.\n\n3. Knowledge Distillation: Train a student model to mimic a teacher model.\n\n4. Block Zoo: Common layers like RNN, CNN, QRNN, Transformer, etc.\n\n5. Model Zoo: End-to-end network templates for common NLP tasks.\n\n6. User Interface: Conven", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The article mentions that they used the Wiktionary corpus for training and testing their models. They also used the deri2016grapheme dataset for evaluating their models.\n\nQuestion: what are the limitations of their approach?\n\nAnswer: The article mentions some limitations of their approach:\n\n1. The models are trained on data in 100 languages, but they do not have data for all languages. This means that the models may not perform well on unseen languages.\n\n2. The models do not use any typological knowledge about the languages. This could limit their performance on unseen", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used in the paper were BIBREF12, which used BERT, RoBERTa and XLNet, and BIBREF13, which used BERT and XLNet.\n\nQuestion: What were the results of the speculation detection and scope resolution tasks?\n\nAnswer: The results of the speculation detection and scope resolution tasks are reported in Figures FIGREF11 and FIGREF12. The XLNet model achieved the best results on both tasks.\n\nQuestion: What were the results of the negation detection and scope resolution tasks?\n\nAnswer:", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "The languages used in the experiment are English, Spanish, Finnish, and Basque. The authors note that the English test set is original, while the rest of the languages are human translated.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: The purpose of the experiment is to analyze the behavior of cross-lingual models trained on translated data, and to understand how this affects the final results. The authors note that the phenomenon of translation artifacts is not specific to NLI but also affects QA.\n\nQuestion: What are the main findings of the experiment?\n\nAnswer:", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "The authors test their method on the task of predicting hashtags for a held-out set of posts. They also show that their model can be used to generate vector representations of entire tweets from characters in the tweet2vec model.\n\nQuestion: How do they generate vector representations of entire tweets from characters in the tweet2vec model?\n\nAnswer: The authors use a Bi-GRU encoder to generate vector representations of entire tweets from characters in the tweet2vec model. The encoder consists of a forward-GRU and a backward-GRU. Both have the same architecture", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained embeddings for the vocabulary. They mention that they use Glove embeddings for the vocabulary.\n\nQuestion: What is the difference between the basic seq2seq model and the model with gated orthogonalization?\n\nAnswer: The basic seq2seq model uses a feedforward neural network to predict the next word in the description conditioned on local characteristics (i.e., words within a field) and global characteristics (i.e., overall structure of the infobox). The model with gated orthogonalization uses an improved attention model with additional regularizer terms", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyResponse was evaluated against some baseline systems. The paper mentions that it was evaluated against a system that uses a transformer model pretrained on ImageNet. It also mentions that it was evaluated against a system that uses a CNN model pretrained on ImageNet. The paper does not provide any details on the baseline systems.\n\nQuestion: What is the main limitation of PolyResponse?\n\nAnswer: The main limitation of PolyResponse is that it does not use explicit semantic representations such as dialogue acts or slot-value ontologies. This means that it cannot handle complex dialogue flows that involve user intent sh", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "The article does not provide details on how they obtain psychological dimensions of people. It mentions that they use the Linguistic Inquiry and Word Count (LIWC) dataset, which contains text from 1,000,000 blog posts. However, it does not explain how they extract psychological dimensions from the text.\n\nQuestion: What is the dataset used for?\n\nAnswer: The dataset is used to generate maps that reflect demographic, linguistic, and psycholinguistic properties of the population represented in the dataset. The maps can be used to study the geographical distributions of psychological", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify the following argument components:\n\n1. Claim: The main point or conclusion that the author is trying to make.\n\n2. Premise: The evidence or support that the author uses to justify the claim.\n\n3. Backing: The reasons or arguments that the author uses to support the premise.\n\n4. Rebuttal: The counterarguments or objections that the author addresses to the premise.\n\n5. Refutation: The reasons or arguments that the author uses to refute the rebuttal.\n\n6. Rebuttal-I:", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "PARENT aligns n-grams of length 1, 2, 3, and 4.\n\nQuestion: What is the correlation between the PARENT metric and the human judgments?\n\nAnswer: The PARENT metric has a high correlation with human judgments, with an average correlation of 0.83.\n\nQuestion: What is the correlation between the BIBREF26 metric and the human judgments?\n\nAnswer: The BIBREF26 metric has a lower correlation with human judgments, with an average correlation of 0.69", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset is 295k conversations, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted.\n\nQuestion: What is the distribution of sentiment polarity in post and comment statements?\n\nAnswer: The distribution of sentiment polarity in post and comment statements is presented in Table TABREF39. The predominant sentiment label of statements is positive and it is the highest for both posts and comments. However, the difference between the amounts of positive and negative statements is higher for the replying comments (34.5% vs", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are:\n\n1. English\n2. French\n3. German\n4. Spanish\n5. Italian\n6. Portuguese\n7. Dutch\n8. Russian\n9. Polish\n10. Czech\n11. Hungarian\n12. Turkish\n\nThe article mentions that the languages are \"typologically diverse\" and includes under-resourced languages like Welsh and Kiswahili.\n\nQuestion: What are the 66 cross-lingual datasets?\n\nAnswer: The 66 cross-lingual datasets are:\n\n1.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The two datasets model is applied to are CMV and Wikipedia.\n\nQuestion: What is the purpose of the CRAFT model?\n\nAnswer: The purpose of the CRAFT model is to forecast conversational derailment in real-time as comments are being posted. It does this by processing comments as they happen and using an order-sensitive context encoder to model inter-comment dependencies.\n\nQuestion: What are the limitations of the CRAFT model?\n\nAnswer: The limitations of the CRAFT model include:\n\n1. It relies on balanced datasets, which may", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were not based on deep learning models. The components were based on more traditional machine learning models such as NLP, ontology matching, and knowledge base population.\n\nQuestion: What is the main goal of the ontology matching component?\n\nAnswer: The main goal of the ontology matching component is to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's general terminology database IATE BIBREF12. The ontology matching component aims to create a linked data knowledge base that", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The article mentions that the data is empirically evaluated using the following methods:\n\n1. Baseline results are provided for automatic speech recognition (ASR), machine translation (MT), and speech translation (ST) using the CoVoST corpus. The results show that the CoVoST corpus provides good quality data for all three tasks.\n\n2. The article provides a detailed analysis of the data, including the number of speakers, accents, and languages covered. It also discusses the quality of the data based on the number of speakers, accents, and languages.\n\n3. The article", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The article states that they use a dual recurrent encoder model that simultaneously encodes audio and text sequences using two recurrent encoder networks. The audio encoder network encodes the audio signal using the ARE model, and the text encoder network encodes the text sequence using the TRE model. The two encoder networks are then combined using a feed-forward neural network layer to generate an attention-application vector that combines the information from both audio and text sequences. The attention-application vector is then passed through a softmax function to predict the emotion class.\n\nQuestion: What is the difference between the ARE", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The paper reports that their model achieved substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into ordinary sentences and treating this synthetic data as additional training data. The results show that the method can produce better results compared to the baselines, demonstrating the effectiveness of adding simplified corpora. The paper does not provide a specific number for the improvement, but the results show that the method can achieve better results compared to the baselines.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The article does not provide information on how many humans evaluated the results. It only mentions that \"the model was evaluated on random 700 examples from our general test set. We picked only examples where a sentence-level translation was better, worse, or equal to the baseline model.\"\n\nThe article does not provide any information on how the evaluations were conducted or how many humans were involved. It is unclear if the evaluations were done by the authors or by other humans.\n\nThe article does not provide any information on the evaluation criteria or metrics used. It is unclear how the evaluators judged the translations.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "The authors define tweets going viral as tweets that were retweeted more than 1000 times by the 8th of November 2016.\n\nQuestion: What is their definition of fake news?\n\nAnswer: The authors define fake news as tweets containing fake news and tweets not containing fake news. They use the categorization provided by Rubin et al. BIBREF7 to identify fake news.\n\nQuestion: What is their hypothesis about the features that might be characteristic of fake news?\n\nAnswer: The authors hypothesize that there are specific features that might be", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "The CNN architecture performs best by itself, as shown in Table TABREF1. The CNN architecture achieves the highest F1 score on the dev set (0.673) and the highest F1 score on the test set (0.673). The CNN architecture also achieves the highest F1 score on the dev set when combined with the BERT architecture (0.673). The CNN architecture also achieves the highest F1 score on the dev set when combined with the LSTM-CRF architecture (0.673). The CNN architecture also achieves the highest F1 score on", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data is collected using crowdsourcing and is described in the article as \"the DeepMine database\". It is the largest public text-dependent and text-prompted speaker verification database in two languages: Persian and English.\n\nQuestion: what are the properties of the data?\n\nAnswer: The data has the following properties:\n\n1. It is text-dependent and text-prompted. This means that the speaker utterances are transcribed from text and the text prompts are used to elicit the utterances.\n\n2. It is collected using crowdsourcing.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The paper mentions that the RQE system uses a combination of machine learning and deep learning methods. The machine learning methods include:\n\n1. Inlineform0: This is a simple IR method that retrieves the top 10 answers from the IR system.\n\n2. Inlineform1: This is a more complex IR method that retrieves the top 10 answers from the IR system and then applies a ranking function based on the scores from the IR system and the RQE classifier.\n\n3. Inlineform2: This is a deep learning method that uses a CNN to extract features from", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset, which is a public dataset consisting of 2197 legitimate users and 2222 spammers. The dataset has been collected by Lee et al. lee2010devils who deployed 60 seed social accounts on Twitter to attract spammers by reporting back what accounts interact with them. The dataset has been slashed because most of the Twitter accounts only have limited number of posts, which are not enough to show their interest inclination. The dataset has been slashed because most of the Twitter accounts only have limited number of posts, which are", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder-decoder architecture.\n\nThe LSTM encoder encodes the full context, including the target form, lemma, and MSD forms. The LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.\n\nThe LSTM encoder-decoder architecture is a common choice for sequence modeling tasks. It allows the model to capture long-range dependencies in the input sequence.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No, they report results on English and Chinese data. They also mention that their approach can be applied to other languages.\n\nQuestion: What is the dataset used for the experiments?\n\nAnswer: The dataset used for the experiments is the FSD dataset, which contains 2,499 news articles in English. They also use the Twitter dataset, which contains 1,000 tweets in English. They also mention that their approach can be applied to other languages.\n\nQuestion: What is the performance of the proposed approach compared to the baselines?\n\nAnswer: The performance of the proposed approach is compared", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (II and IV) from each of the folds 1-3, i.e., $|{\\mathcal {M}}|=6$ models. The ensemble+ achieved a F1 score of 0.673 on the dev set. The ensemble+ was built by taking the two best configurations (II and IV) on dev (internal) set and combining them using majority voting. The ensemble+ achieved a boost in recall and thus an improved F1 score on dev (external).", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a strong baseline model that achieved good BLEU scores on the in-domain data. The authors did not provide details about the specific model, but it is likely to be a strong NMT model that has been fine-tuned on the in-domain data.\n\nQuestion: what was the proposed method?\n\nAnswer: The proposed method was a multistage fine-tuning approach that combined two types of transfer learning: domain adaptation for Ja INLINEFORM0 En and Ru INLINEFORM1 En and multilingual transfer for Ja INLINEFORM2 Ru and Ru INLINEFORM3", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The highest recall score was 0.2862 for the system UNCC_QA_1.\n\nQuestion: What was their lowest F-measure score?\n\nAnswer: The lowest F-measure score was 0.0786 for the system UNCC_QA_1.\n\nQuestion: What was the difference in recall score between the two systems?\n\nAnswer: The difference in recall score between the two systems was 0.2086.\n\nQuestion: What was the difference in F-measure score between the two systems?\n\nAnswer: The difference in F", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores integrating semantic similarity measures into second-order co-occurrence vectors. This involves using semantic similarity scores to calculate a second-order co-occurrence vector for each term in a pair of terms. The paper also explores integrating semantic similarity scores into word embeddings.\n\nQuestion: What is the goal of the paper?\n\nAnswer: The goal of the paper is to propose a method for quantifying the similarity and relatedness between two terms that integrates pair-wise similarity scores into second-order co-occurrence vectors. The authors aim to improve the correlation between the similarity measures and human", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The paper mentions that they use a \"pre-ordering\" step to match the words before reordering them. They explain that this is done by \"considering the word order divergence between the source and the assisting language\" and \"pre-ordering the source language sentences to match the word order of the target language\". They also mention that they use a \"pre-ordering\" system called CFILT-preorder to do this.\n\nIn summary, they pre-order the source language sentences to match the word order of the target language before reordering them. This is done to address the word", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No, the paper does not explore extraction from electronic health records. The paper focuses on extraction from biomedical corpora, which includes text from scientific literature.\n\nQuestion: What are the main challenges in relation extraction in the biomedical domain?\n\nAnswer: The main challenges in relation extraction in the biomedical domain are:\n\n1. Poor performance of NLP tools in the biomedical domain compared to general domain NLP tools. This is due to the complexity of the biomedical domain, with many specialized terms and concepts.\n\n2. Incorrect extra", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts used for annotation were crowdworkers who consented to participate in the study. The authors did not identify the specific experts.\n\nQuestion: What is the scope of the PrivacyQA dataset?\n\nAnswer: The PrivacyQA dataset contains 573 questions and 3500 expert annotations of relevant answers. The questions are based on real user search engine queries that were filtered to identify questions that were difficult to answer. The answers are provided by experts who were crowdworkers who consented to participate in the study.\n\nQuestion: What are the factors that make a", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The article uses two different models for painting embedding and language style transfer. For painting embedding, it uses a sequence-to-sequence model with a pointer network. This model takes an image as input and generates a sequence of words as a poem. For language style transfer, it uses a seq2seq model with global attention. This model takes a source sentence and generates a target sentence in the same style as the source sentence.\n\nThe seq2seq model with pointer network is used for painting embedding because it can generate a sequence of words that describe the image. The seq2seq model with global attention is used for language style transfer because it", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The answer is \"unanswerable\". The article does not provide any information to answer this question.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "The authors hypothesize that humans' robustness to noise is due to their general knowledge. They argue that humans are able to understand complex information and make sense of it, while MRC models are not. They propose that this is because humans have access to general knowledge that helps them understand the context and meaning of the information, while MRC models do not.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The authors addressed the following cyberbullying topics:\n\n1. Formspring: They used Formspring dataset to train and evaluate their models. Formspring is a now-defunct social media platform that was used for cyberbullying.\n\n2. Wikipedia: They used Wikipedia dataset to train and evaluate their models. Wikipedia is a collaborative knowledge repository that was used for cyberbullying.\n\n3. Twitter: They used Twitter dataset to train and evaluate their models. Twitter is a microblogging platform that was used for cyberbullying.\n\n4. BLSTM: They used BL", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The new context representation for convolutional neural networks is described in the article as follows:\n\n\"We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. While early research mostly focused on support vector machines or maximum entropy classifiers, we propose to use convolutional filters of different sizes for CNNs. For NLP tasks, the CNN with a ranking layer can combine all intermediate hidden layers for their classification.\"\n\nIn", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The article mentions that the dataset contains three major classes: Person (PER), Location (LOC), and Organization (ORG). This means there are three different types of entities in the dataset.\n\nQuestion: What is the dataset size?\n\nAnswer: The article mentions that the dataset contains 6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus. The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively. The dataset is in standard", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The article does not provide a quantitative measure of the quality of the annotated data. However, it does mention that the experts produced higher quality annotations than the crowd workers. The article also states that the experts were more consistently annotated by both experts and crowd workers. Based on these observations, it is likely that the expert annotations are of higher quality than the crowd annotations.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The article states that the gender representation in the analyzed corpora is imbalanced, with 65% of the speakers being men and 35% being women. The authors also note that the gender bias in ASR performance is likely due to this imbalance.\n\nQuestion: What is the impact of gender bias in ASR performance?\n\nAnswer: The article states that gender bias in ASR performance can lead to re-implementing and reinforcing discrimination already existing in our societies. The authors also note that gender bias in ASR performance can be caused by the data used to", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the Multi30K dataset. The Multi30K dataset is a large-scale dataset that contains 29,000 instances for training, 1,014 for development, and 1,000 for test. It contains 29,000 instances for training, 1,014 for development, and 1,000 for test. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. The dataset contains", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The article compares the proposed model to the following strong baselines:\n\n1. BIBREF24: A Gaussian-masked directional multi-head attention scorer that uses bi-affine attention.\n\n2. BIBREF26: A gated recursive neural network (GRNN) for Chinese word segmentation.\n\n3. BIBREF30: A neural model that incorporates dictionaries into neural networks for Chinese word segmentation.\n\n4. BIBREF33: A joint model that performs segmentation, part-of-speech tagging, and chunking simultaneously", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The article mentions that the proposed approach uses probabilistic models for keyword discovery and expectation estimation. It does not specify the exact classifiers used.\n\nQuestion: What is the dataset used for experiments?\n\nAnswer: The article mentions that the experiments are conducted on two datasets: CyberAttack and PoliticianDeath. The CyberAttack dataset contains 80k tweets and the PoliticianDeath dataset contains 20k tweets.\n\nQuestion: What is the evaluation metric used?\n\nAnswer: The article mentions that the experiments are evaluated using accuracy and area under the precision-recall curve (", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use BIBREF17 and BIBREF18 for entity-level sentiment analysis and BIBREF17, BIBREF19, CogComp-NLP BIBREF20, and Stanford NLP NER BIBREF21 for named-entity recognition.\n\nQuestion: What is the dataset used for the experiments?\n\nAnswer: The dataset used for the experiments is a 1,000-tweet dataset provided by BIBREF2 that contains the named-entities labels and entity-level sentiments for each of the four 2016 presidential", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are performed on the SQuAD dataset BIBREF3.\n\nQuestion: What are the results and analysis of the experiments?\n\nAnswer: The results and analysis of the experiments are as follows:\n\n1. The proposed model achieves state-of-the-art performance across all datasets compared to baseline models that only consider unstructured sentences.\n\n2. The proposed model can extract long term dependency words and generate diverse questions with different structured answer-relevant relations.\n\n3. The proposed model can generate questions that are more informative and faithful compared to baseline models that only consider", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The article mentions several existing approaches for learning geographic location embeddings:\n\n1. Bag-of-words models: These use the bag-of-words representation of Flickr tags to learn location embeddings. They do not consider any structured information.\n\n2. GloVe models: These use the GloVe model to learn location embeddings. They consider both bag-of-words representations of Flickr tags and structured information from CORINE land cover classes.\n\n3. EGEL-Tags: This approach uses the EGEL model to learn location embeddings", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention in the joint model. The joint model includes two different layers: the shared layer and task specific layer. The shared layer is almost identical to the lower layers of SAN, which has additional binary classifier added in the model justifying whether the question is unanswerable. The unanswerable classifier is a pair-wise classification model that predicts a label indicating whether the given pair of a passage and a question is unanswerable. The two models share the same lower layer to save the number of parameters, and separate the task specific layer to avoid overfitting. The attention function is used to compute the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "The article mentions that they used the following datasets for evaluation:\n\n1. CSAT dataset for CSAT classification\n2. 20 newsgroups dataset for 20 newsgroups classification\n3. Fisher dataset for Fisher topic identification\n\nThey also mention that they used the following pre-trained BERT features:\n\n1. 53160 words vocabulary set available in the datasets website\n2. 1103 calls for testing\n3. 2866 calls for training\n4. 362 calls for validation\n\nSo in summary, they used", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "The sentiment classification dataset used is the IMDb movie review dataset BIBREF17 .\n\nQuestion: What is the experimental setup?\n\nAnswer: The experimental setup includes:\n\n1. Optimization by stochastic gradient descent (SGD) without momentum.\n\n2. Learning rate of 1 for 6 epochs, then decayed by 0.95 for each subsequent epoch.\n\n3. 72 epochs in total.\n\n4. 20 examples per batch.\n\n5. 24 examples per timestep.\n\n6. 20,", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No, the tasks were not evaluated in any previous work. The BERT models were trained on different and larger corpora (covering both wikipedia and books) and allowed to access the suffix of the sentence in addition to its prefix. The high performance numbers indicate that the purely attention-based BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better. The BERT-Base model outperformed the BERT-Large model on many of the syntactic test", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "The article mentions that the dataset of 1,000 randomly selected tweets contains more than 1,000 tweets. However, it does not provide any information on the distribution of tweets across different sentiment labels. It is unclear if the dataset is balanced in terms of the number of tweets for each sentiment label.\n\nThe article also mentions that the dataset contains more than 1,000 tweets, but it does not provide any information on the number of tweets for each candidate. It is unclear if the dataset is balanced in terms of the number of tweets for each candidate.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian matrix of the invertible transformation is triangular with all ones on the main diagonal. This means that the transformation is invertible and the Jacobian matrix is a valid inverse.\n\nQuestion: What is the Gaussian HMM?\n\nAnswer: The Gaussian HMM is a generative model that uses Gaussian distributions to model the latent variables. It is a Markov-structured model that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion.\n\nQuestion: What is the DMV-structured syntax model", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema looks like this:\n\n1. Identify the question and the passage.\n\n2. Identify the supporting facts. These are the facts that are relevant to answering the question. They can be facts from the question, the passage, or both.\n\n3. Identify the distractors. These are the facts that are not relevant to answering the question. They can be facts from the question, the passage, or both.\n\n4. Identify the lexical features. These are the linguistic features that are relevant to answering the question. They can be words, phrases,", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The sizes of both datasets are not explicitly stated in the article. However, the authors mention that the WikiSmall dataset has 89,042 sentence pairs and the WikiLarge dataset has 296,402 sentence pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are:\n\n1. Vanilla ST baseline: The vanilla ST baseline has only a speech encoder and a decoder. It is trained from scratch on the mixture of ST-TED and TED-LIUM2 corpus BIBREF14.\n\n2. Decoder pre-training: The decoder is initialized from an MT model and fine-tuned on in-domain data.\n\n3. Encoder-decoder pre-training: Both the encoder and decoder are pre-trained.\n\n4. Encoder pre-training: The", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "The paper studies English natural language.\n\nQuestion: What is the purpose of the paper?\n\nAnswer: The paper aims to propose a method of incorporating cost-sensitivity into BERT to allow for better generalisation and additionally provide a simple measure of corpus similarity to determine when this method is likely to be useful.\n\nQuestion: What is the main finding of the paper?\n\nAnswer: The main finding is that incorporating cost-sensitivity into BERT and using a simple measure of corpus similarity can help improve the generalisation of the model to less similar data.\n\nQuestion: What are", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The experiment uses two models:\n\n1. A linear SVM trained on word unigrams.\n\n2. A bidirectional Long Short-Term Memory (BiLSTM) model.\n\nThe BiLSTM model is adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead of sentiment.\n\nThe CNN model is based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.\n\nHope this helps! Let me know if you have any other questions", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The answerability of questions on Quora is measured by the usefulness of the answer. The answerability of questions is determined by the usefulness of the answer.\n\nAnswer: The answerability of questions on Quora is measured by the usefulness of the answer. The answerability of questions is determined by the usefulness of the answer.\n\nAnswer: The answerability of questions on Quora is measured by the usefulness of the answer. The answerability of questions is determined by the usefulness of the answer.\n\nAnswer: The answerability of questions on Quora is measured by the usefulness of the answer.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The paper mentions that the pretrained word embeddings used were GloVe, Edinburgh, and Brown clusters. The paper also mentions that the authors implemented 14 different feature extractors, which can be clubbed into 3 major categories:\n\n1. Lexicon Features: AFINN BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, NRC Word-Emotion Association Lexicon BIBREF7, NRC Hashtag Emotion Lexicon BIBREF8, NRC Hashtag Sentiment Lex", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The paper reports the results of their experiments on the new dataset. The results show that their personalized models can generate more coherent and personalized recipes compared to the baseline models. The personalized models also achieve higher scores on coherence metrics and human evaluations. The paper concludes that personalized models can generate more coherent and personalized recipes compared to baseline models.\n\nQuestion: How did they evaluate the generated recipes?\n\nAnswer: The paper reports the results of their human evaluations on generated recipes. The evaluators found that the generated recipes were more coherent and", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is:\n\n1. Sentiment preservation reward: INLINEFORM0\n\n2. Content preservation reward: INLINEFORM1\n\n3. Irony accuracy reward: INLINEFORM2\n\n4. Irony reward: INLINEFORM3\n\n5. Sentiment accuracy reward: INLINEFORM4\n\n6. BLEU score: INLINEFORM5\n\n7. Irony accuracy: INLINEFORM6\n\n8. Human evaluation: INLINEFORM7\n\n9. Geometric mean: INLINEFORM8\n\n10", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate several limitations of their model:\n\n1. The model requires a large parallel dataset of images and poems to train effectively. This dataset does not exist for Shakespearean prose. The authors suggest expanding the style transfer dataset to address this limitation.\n\n2. The model relies on a seq2seq model with global attention, which performs better in practice than a model with local attention. This may not generalize to other seq2seq models.\n\n3. The model requires a large number of hidden units to achieve good performance. This may not scale well to smaller models.\n\n4. The model", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "The article mentions that they compared their model to the following systems:\n\n1. Affective Text dataset\n2. Fairy Tales dataset\n3. ISEAR dataset\n\nThey also mention that they used the development set of the Affective Text dataset to evaluate their model.\n\nHope this helps! Let me know if you have any other questions.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The article reports the following distribution results:\n\n1. The number of retweets was higher for fake news tweets compared to non-fake news tweets.\n\n2. The number of friends/followers was higher for fake news tweets compared to non-fake news tweets.\n\n3. The number of mentions was lower for fake news tweets compared to non-fake news tweets.\n\n4. The number of URLs was higher for fake news tweets compared to non-fake news tweets.\n\n5. The ratio of friends/followers was higher for fake news", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF31 . The dataset contains 1,268 unique English hashtags from 1,108 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF31 . The hashtags are manually segmented and the segmentation is verified by the annotators.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 12,594 unique English hashtags from 1,108 randomly selected tweets", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The DeepMine database contains speech samples from Persian speakers with different accents. The accents include Tehran, Isfahan, Shiraz, Mashhad, and other regional accents. The database also contains speech samples from English speakers with different accents.\n\nQuestion: what is the size of the database?\n\nAnswer: The DeepMine database contains 5000 utterances from 50 Persian speakers and 5000 utterances from 50 English speakers. The database also contains 5000 utterances from 50 imposter", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the semantic meaning of words. It encodes the semantic structure of words, rendering vector angles as a useful metric to show meaningful similarities between words.\n\nFor example, the word \"cat\" has a vector angle of 0.000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The baseline model used is Random Forests (RF) and Support Vector Machines (SVM).\n\nQuestion: What is the dataset used for the experiments?\n\nAnswer: The dataset used for the experiments is directly extracted from the Wikipedia entity pages and their revision history. The dataset consists of the following:\n\nEntity Classes: 27 classes including INLINEFORM0 classes with at least one news reference.\n\nNews Articles: 73,734 news articles.\n\nEntity Profiles: 73,734 entity profiles.\n\nSection Templates: 27 section", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No, SemCor3.0 is a manually annotated corpus of English language data that is specific to the WordNet sense inventory. It is not representative of English language data in general.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "Augmented LibriSpeech dataset is a large-scale dataset consisting of 100,000 hours of speech data from 2,000 speakers. It is a multilingual dataset with 11 languages: English, French, German, Spanish, Italian, Dutch, Portuguese, Russian, Mandarin, Arabic, and Turkish. The dataset is split into training, development, and test sets. The training set has 80,000 hours of speech data, the development set has 10,000 hours, and the test set has 10,00", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The article mentions that they used the fine-grained sentiment classification dataset from the SemEval-2016 \"Sentiment Analysis in Twitter\" task BIBREF16 . They also used the ternary sentiment classification dataset from the same task.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The fine-grained dataset contains tweets with binary, ternary, and fine-grained sentiment labels. The ternary dataset contains tweets with binary, ternary, and fine-grained sentiment labels. The ternary dataset is more challenging because it", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "The paper does not specify the size of the BERT model used. However, it mentions that they fine-tune the pre-trained BERT model on the WSD task. This suggests that they may have used a larger BERT model for fine-tuning.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "The automatically constructed datasets are subject to quality control in the following ways:\n\n1. The datasets are constructed using a combination of human expert knowledge and machine learning models. The human expert knowledge is used to identify the target concepts and clusters, and the machine learning models are used to generate the datasets. The human expert knowledge is assumed to be correct, but the machine learning models are fallible and error-prone.\n\n2. The datasets are validated using crowd-sourcing experiments to check for agreement across probes and human scores. The crowd-sourcing experiments show high agreement across probes and human scores.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "The images in the ShapeWorldICE dataset are from a variety of domains, including objects, scenes, and abstract shapes. The dataset does not specify a specific domain.\n\nQuestion: What is the purpose of the ShapeWorldICE dataset?\n\nAnswer: The purpose of the ShapeWorldICE dataset is to enable researchers to evaluate the performance of image captioning models on a variety of tasks that require different aspects of performance. The dataset provides a controlled environment for evaluating models on tasks that involve grammaticality, truthfulness, and diversity.\n\nQuestion: How does the ShapeWorldICE dataset", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The article reports the performance of their model on emotion detection. The results are summarized in Table 2. The best model achieved a micro-average f-score of 0.368 on the development set. The best model (B-M) achieved a micro-average f-score of 0.412 on the development set. The best model (B-M) achieved a micro-average f-score of 0.392 on the test set. The best model (B-M) achieved a micro-average f-score of 0.389", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is described in the article as follows:\n\n\"To capture this interesting property, we also incorporate word position knowledge into our structured prediction model. Exemplified by the second sentence “Some diets cause a gut reaction,\" the second tag sequence INLINEFORM0 indicates that the current word appears before the pun in the given context. In the same way, the first tag sequence INLINEFORM1 indicates that the current word appears after the pun. We thus observe that a pun tends to appear at the second half of a context. Specifically, based on the statistics, we found that sentences with a pun that", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No, Arabic is not one of the 11 languages in CoVost. The article states that the 11 languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese. Arabic is not included in this list.\n\nQuestion: What is the average number of speakers per language in CoVost?\n\nAnswer: The article does not provide an average number of speakers per language in CoVost. However, it does state that the corpus has over 11,000 speakers and over 60 acc", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The authors define robustness as the ability of a model to handle unbalanced labeled features and unbalanced datasets. They argue that incorporating neutral features, maximum entropy regularization terms, and KL divergence terms can improve the robustness of a model.\n\nThey provide the following examples of how these terms can improve robustness:\n\n1. Incorporating neutral features can help the model handle unbalanced labeled features. The model can learn from neutral features even if the labeled features are unbalanced.\n\n2. The maximum entropy regularization term can help the model handle unbalanced datasets", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "The other sentence embeddings methods evaluated are:\n\n1. InferSent: A fine-tuned version of BERT that uses a siamese / triplet network architecture.\n\n2. Universal Sentence Encoder: A pre-trained version of BERT that uses a cosine-similarity based objective function.\n\n3. SBERT: A fine-tuned version of BERT that uses a siamese / triplet network architecture and a logistic regression classifier.\n\n4. RoBERTa: A pre-trained version of BERT that uses", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The method's improvements of F1 for NER task for English and Chinese datasets are as follows:\n\nEnglish OntoNotes4.0:\n\n- The method achieves an F1 score of 84.67 when $\\alpha$ is set to 0.6, which is a significant improvement over the baseline F1 score of 55.57.\n\n- The method achieves an F1 score of 68.44 when $\\alpha$ is set to 0.6, which is a significant improvement over the baseline F1 score of 54.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "The conflict method is tested on two tasks:\n\n1. Quora duplicate question pair detection: The conflict method is used to detect duplicate question pairs in the Quora dataset. The conflict method is trained to detect pairs where one sequence is marked as duplicate in the previous attention model but the other sequence is marked as non-duplicate in the conflict model.\n\n2. Ranking questions in Bing's People Also Ask: The conflict method is used to classify whether a question is a high-click question or not based on the click logs. The conflict method is trained to detect questions that are marked as high-click in", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "The article mentions that they compared against the following baselines:\n\n1. Syntactic tree-LSTM: This is a model that uses a syntactic tree to guide the composition of semantic units. It is a baseline that does not use any tag information.\n\n2. Latent syntax tree-LSTM: This is a model that uses a latent syntax tree to guide the composition of semantic units. It is a baseline that does not use any tag information.\n\n3. Gumbel tree-LSTM: This is a model that uses a Gumbel softmax function to", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model. This model is used to identify the KB relations between the question and the entities in the knowledge base. The relation detection model is based on the entity linking step, which identifies the entities in the question and the knowledge base. The relation detection model then uses the entity linking results to identify the KB relations between the question and the entities in the knowledge base.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the recipe name model, the prior technique model, and the prior name model. These models are used to generate recipes based on the recipe name and user preferences.\n\nQuestion: What is the dataset used for the personalized models?\n\nAnswer: The dataset used for the personalized models is a dataset of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018).\n\n", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The paper mentions several methods to find examples of biases and unwarranted inferences in the Flickr30K dataset:\n\n1. Manual categorization of images based on ethnicity, age, and gender. This allows the researchers to identify which images contain biased descriptions.\n\n2. Part-of-speech tagging to identify which adjectives are used to describe entities. This can help identify which entities are described in a biased way.\n\n3. Enriching the Flickr30K dataset with entity information to create a coreference graph. This allows the", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "The article explores the Winograd Schema Challenge, a challenge for machine translation programs to solve a Winograd schema. The challenge involves finding the correct referent for the ambiguous pronoun in a sentence. The challenge is administered by commonsensereasoning.org and sponsored by Nuance Inc. It was offered for the first time at IJCAI-2016 (Morgenstern, Davis, and Ortiz, in preparation); the organizers plan to continue to offer it roughly once a year.\n\nThe challenge involves finding the correct referent for the ambiguous pron", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "The authors experimented with the following models:\n\n1. CAS-LSTM: This is the proposed model that combines stacked LSTMs with cell states. It uses not only hidden states but also cell states from the previous layer, for the purpose of controlling the vertical information flow in a more elaborate way.\n\n2. Stacked LSTM: This is a baseline model that uses only hidden states. It does not use cell states.\n\n3. Stacked LSTM with different INLINEFORM0: This is a variant of the baseline model that uses different INLINEFORM", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they report results on both English and Polish data. They also mention that their approach can be extended to other word embedding algorithms.\n\nQuestion: What is the limitation of the proposed approach?\n\nAnswer: The limitation is that it cannot perfectly handle polysemy. Since related words are clustered together, sufficiently well-connected words that do not meaningfully belong along with others will be appropriately \"pulled away\" from that group by several words, against the less effective, inappropriate pull of a particular word. Even though polish with lower connectedness to a concept group belongs where it is, it is", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including:\n\n1. A multi-class multi-label classifier based on sentence classification using the SVM and ADWS kernels.\n\n2. A multi-class multi-label classifier based on clustering the sentences using the cosine similarity between word embeddings.\n\n3. A sentence-based summarization algorithm using the Carrot2 Lingo and CLUTO BIBREF19 clustering algorithms.\n\n4. An ILP-based summarization algorithm using the ILP formulation described in Table TABREF36.\n", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was using context-free word embeddings to represent the posts. This approach was shown to be insufficient for the task of inferring the optimal context length. The proposed approach of using context-aware word embeddings and attention models to infer the optimal context length is a significant improvement over the previous state of the art.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The hierarchical variants of MPAD are the least impactful component. The vanilla MPAD architecture achieves the best results on all datasets, while the hierarchical variants only achieve better results on some datasets. The hierarchical variants also have higher complexity and computational cost compared to the vanilla MPAD architecture.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the DTA corpus BIBREF11, which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 contains 26 million sentences published between 1850-1899. The corpus version used in the task has the following format: \"year [tab] lemma1 lemma", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The article mentions that they experiment with 7 Indian languages: Hindi, Kannada, Malayalam, Bengali, Telugu, Tamil, and Marathi.\n\nQuestion: What is the dataset used for the experiments?\n\nAnswer: The dataset used for the experiments is a 635-hour audio dataset collected from the All India Radio news channel. The dataset contains 635 hours of clean audio data for 7 Indian languages: Hindi, Kannada, Malayalam, Bengali, Telugu, Tamil, and Marathi.\n\nQuestion", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The paper reports the model performance on target language reading comprehension in Table 2. The results show that the model achieves competitive performance on target language reading comprehension, with F1 scores ranging from 44.1 to 53.8. The model also achieves better performance on target language than on English, indicating that the model has learned language-agnostic representations that can be transferred to the target language.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The article states that the proposed model ALOHA achieves a performance close to humans, but the difference in performance between ALOHA and the baselines is not explicitly stated. The article does not provide a quantitative comparison of the performance of ALOHA and the baselines.\n\nThe article does provide a qualitative analysis of the results, stating that ALOHA is able to retrieve the correct response of a target character by its HLAs. However, the article does not provide a quantitative comparison of the performance of ALOHA and the baselines.\n\nThe article also states that the proposed model ALOHA", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "According to the article, ARAML achieves better performance than the baselines in terms of forward perplexity, reverse perplexity, and Self-BLEU. The authors report that ARAML achieves a forward perplexity of 1.86, which is 0.02 higher than the best baseline. The reverse perplexity of ARAML is 0.02 lower than the best baseline, indicating that ARAML can better fit the data distribution. The Self-BLEU score of ARAML is 0.02 higher than the best baseline,", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the hate speech detection task. They note that the model misclassifies some samples as hate speech when they are actually offensive or neutral, and that this misclassification is due to biases in the data collection and annotation process. They also note that some samples contain offensive words and slurs that are oversampled in the dataset, which may lead to misclassifications.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, other baselines were tested to compare with the neural baseline. The article mentions that the authors implemented three baselines:\n\n1. A human performance baseline that answers questions based on the privacy policy text.\n\n2. A word count baseline that identifies relevant sentences based on the question and privacy policy text.\n\n3. A CNN neural encoder that identifies relevant sentences based on the question and privacy policy text.\n\nThe authors also mention that they implemented two BERT-based baselines that identify relevant sentences based on the question and privacy policy text.\n\nIn summary,", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is 6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus. The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the size of the entities?\n\nAnswer: The entities are 72782 unique words.\n\nQuestion: What is the size of the embeddings?\n\nAnswer: The embeddings are 300 dimensions.\n\nQuestion: What", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The method improvements of F1 for paraphrase identification are as follows:\n\n1. The proposed DSC loss performs robustly on all the three datasets.\n\n2. Experiments show that the proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively for English datasets.\n\n3. The proposed method achieves F1 scores of +0.97 and +2.36 on Chinese datasets, which are significantly higher than BERT-MRCBIBREF38.\n\n4. The proposed method sets new S", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article uses the BIBREF0 dataset, which contains electroencephalography (EEG) data from 12 participants who read a chapter of Harry Potter and the Sorcerer's Stone. The dataset also contains eye-tracking data, self-paced reading time data, and electroencephalography (ERP) data.\n\nQuestion: What are the six ERP components?\n\nAnswer: The six ERP components are:\n\n1. N400: Negative relative to a baseline (the baseline is typically recorded just before a word is presented at an electrode", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of imagined speech tasks, including imagined vowels, imagined words, and imagined syllables. The subjects were asked to imagine producing the speech sounds corresponding to the task.\n\nQuestion: What was the experimental design of the study?\n\nAnswer: The study had a within-subject design, with each subject being presented with all the tasks. The subjects were presented with each task 11 times, for a total of 125 trials per subject.\n\nQuestion: What was the classification accuracy of the proposed method?\n\nAnswer: The proposed method achieved", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "The paper mentions that the following baselines are used for evaluation:\n\n1. Pointer-Gen+ARL-SEN\n2. Pointer-Gen+ARL-SEN+RL-SEN\n3. Pointer-Gen+ARL-SEN+RL-ROUGE\n4. Pointer-Gen+ARL-SEN+RL-ROUGE+ARL-SEN\n5. Pointer-Gen+ARL-SEN+RL-ROUGE+ARL-SEN+RL-SEN\n\nThe paper also mentions that the Pointer", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The article mentions that they use five feature engineering based machine learning classifiers:\n\n1. Naive Bayes (NB): Multinomial NB with additive smoothing constant 1\n\n2. Logistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization\n\n3. Support Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function\n\n4. Random Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees\n\n5. Gradient Bo", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The article mentions two language model architectures:\n\n1. A bi-directional language model to augment the sequence to sequence encoder. This model uses self-attention and the uni-directional model contains an encoder and a decoder.\n\n2. A uni-directional model to input contextualized embeddings either to the encoder or the decoder. This model uses self-attention and fine-tunes the language model when training the sequence to sequence model.\n\nThe article does not provide specific details about the language model architectures.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted using the DSC loss function. The DSC loss is a soft version of the F1 score that penalizes incorrect predictions and rewards correct predictions. The weights are adjusted based on the DSC loss during training.\n\nQuestion: What is the effect of hyperparameters?\n\nAnswer: The effect of hyperparameters depends on the dataset and task. The hyperparameters $\\alpha$ and $\\beta$ control the tradeoff between precision and recall. The higher the value of $\\alpha$, the more the model focuses on precision. The higher the value of $\\beta$, the more the model focus", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies in the article are:\n\n1. A method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen.\n\n2. An exploration algorithm such as Go-Explore BIBREF9 that leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Ex", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of the following components:\n\n1. A set of monolingual models, one for each language. These models are trained on the monolingual data for that language.\n\n2. A set of crosslingual models, one for each language pair. These models are trained on the parallel corpora for that language pair.\n\n3. A set of crosslingual latent variables. These variables capture correlations between roles in different languages.\n\n4. A set of monolingual latent variables. These variables capture correlations between roles in the monolingual data.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The article mentions that non-standard pronunciation is identified by comparing the transcription with the original audio recording. The transcription is then annotated to indicate which phonemes are not found in the original audio.\n\nQuestion: What is the potential impact of this resource?\n\nAnswer: The potential impact of this resource is that it could be used to build language systems for Mapudungun, an under-represented language of the Americas. The large amount of data and high-quality annotations could help overcome some of the challenges faced when building language systems for low-resource languages.\n\nQuestion: What", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of word recognition model that uses a single-layer BiLSTM to map each word to a sequence of semichars. The semichars are a fixed set of characters that represent the word, and the BiLSTM is trained to recognize the semichars as words.\n\nFor example, the word \"hello\" would be mapped to the semichars \"h\", \"e\", \"l\", \"l\", \"o\", which the BiLSTM would learn to recognize as the word \"hello\".\n\nThe semicharacter architecture is used in the Sc", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The article explores 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what are the main conclusions?\n\nAnswer: The main conclusions are:\n\n1. Feature-based models enriched with external lexicons perform as well as or better than bi-LSTM-based models.\n\n2. The best performing feature-based models are those that use both local standard features (for example the current word itself and its", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "The NCEL approach is effective overall, as shown by the results on the benchmark datasets. The NCEL model achieves competitive results compared to the state-of-the-art collective entity linking methods, and outperforms them in some cases. The NCEL model is also end-to-end trainable, which makes it more efficient to train.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "The data is de-identified. The article states that the data is de-identified using the following methods:\n\n1. Removing all identifying information from the data, including names, addresses, phone numbers, and other identifiers.\n\n2. Using a de-identification tool to remove all identifying information from the data.\n\n3. Using a de-identification tool to remove all identifying information from the data and then removing the data from the tool.\n\n4. Using a de-identification tool to remove all identifying information from the data and then removing the data from the tool", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the Felice2014a system, which was trained using the FCE dataset and evaluated on the CoNLL 2014 dataset.\n\nQuestion: What were the two approaches for generating artificial errors?\n\nAnswer: The two approaches were:\n\n1. Using textual patterns learned from an annotated corpus to insert errors into correct input text.\n\n2. Formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences.\n\nQuestion: How were the two approaches evaluated?\n\nAnswer: The two approaches were evaluated on", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2 database, which is a large clinical database containing de-identified electronic health records. The i2b2 database is available for research purposes and contains clinical notes from multiple hospitals.\n\nQuestion: what is the purpose of the \"ELMo\" embeddings?\n\nAnswer: The ELMo embeddings are pre-trained word embeddings that were fine-tuned on the i2b2 clinical notes. They provide a vector representation for each word in the clinical notes, which helps the NER model learn", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the model to focus on generating the summary without being distracted by the masked words. This helps the model generate more fluent and natural summaries.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article mentions that they use the PPDB dataset for their experiments. PPDB is a dataset of 100 million tweets collected from 2010 to 2014. It contains 100 million tweets, 100 million hashtags, and 100 million mentions.\n\nQuestion: What is the objective function of the models?\n\nAnswer: The objective function of the models is to predict the labels of the tweets. The labels can be sentiment, hashtags, or other labels. The models are trained to predict the labels based on the objective", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The article mentions that TF-IDF features are used to extract keywords from pathology reports. TF-IDF stands for Term Frequency-Inverse Document Frequency, which is a weighting scheme in information retrieval and text mining. It is used to identify important words within reports that summarize its content. The article also mentions that LDA (Latent Dirichlet Allocation) is used to group these keywords into different topics. LDA is an unsupervised machine learning technique that allows grouping words into topics based on their co-occurrence in documents.\n\nIn summary, the article uses TF", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated by a team of annotators who read the tweets and classified them as either containing evidence of depression or not. The annotators were trained on the hierarchical model of depression-related symptoms and were instructed to classify tweets based on the presence or absence of depression-related symptoms. The annotators were also instructed to use the LIWC categories to identify depression-related symptoms.\n\nThe dataset contains 9,300 annotated tweets, with 9,473 annotations for 9,300 tweets. Each", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight NER tasks evaluated on are:\n\n1. Biomedical NER\n\n2. Covid-19 QA\n\n3. PubMed+PMC\n\n4. SQuAD\n\n5. SQuAD2.0\n\n6. SQuADv1.1\n\n7. SQuADv2.0\n\n8. SQuADv2.1\n\nThe paper mentions that they also evaluated on the BioBERTv1.0 dataset, but it is not explicitly mentioned which tasks they evaluated on.\n\nHope this helps! Let me", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated using the machine translation platform Apertium BIBREF5. The platform uses a combination of statistical and rule-based translation models to generate translations. The training data was translated from English to Spanish.\n\nQuestion: How were the models trained?\n\nAnswer: The models were trained using the following methods:\n\n1. Feed-forward neural networks: The feed-forward neural networks were trained using the Keras BIBREF9 library. The models were trained using the Adam optimizer and the MSE loss function.\n\n2. Long short-term memory (LSTM) networks", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "The system used a content-based classifier in conjunction with two feature selection methods: (1) Information Gain Ratio (IGR) and (2) Aggressive Feature Ranking (AFR). The content-based classifier used the top 95% of all the features as features, while the IGR and AFR feature selection methods used the top 5% of all the features as features.\n\nQuestion: What is the significance of the gender dominance ratio in the industries?\n\nAnswer: The gender dominance ratio in the industries is significant because it shows that regardless of their", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for this task was a very simple logistic regression classifier with default parameters, where the target sentence was followed by the title of the article.\n\nQuestion: What was the approach of Team MIC-CIS?\n\nAnswer: Team MIC-CIS used BERT, but they paid special attention to the imbalance of the data and the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The article compares their proposed approach with the following baselines:\n\n1. Conditional Random Fields (CRF) approach based on word embeddings.\n\n2. Conditional Random Fields (CRF) approach based on character embeddings.\n\n3. Conditional Random Fields (CRF) approach based on pun detection and location tasks as a single sequence labeling problem.\n\n4. A rule-based approach for pun detection and location based on the position within the context and part of speech features.\n\n5. A supervised approach for pun detection based on word sense knowledge.\n", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The article mentions that the political bias of different sources is included in the model by considering the political bias of news sources in the dataset. The authors state that they \"employed stratified shuffle split cross validation (with 10-fold stratified shuffle split cross validation) to evaluate performances\" and \"we can notice that both Q and M layers alone capture adequately the discrepancies of the two distinct news domains in the United States as they obtain good results with AUROC values in the range 75%-86%; these are comparable with those of the multi-layer approach which,", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from the following sources:\n\n1. Ancient-Modern Chinese Corpus (AMCC) - This is a corpus of 35K aligned bilingual paragraphs from 28K aligned ancient Chinese paragraphs and 6.5K aligned modern Chinese paragraphs. It contains 517K aligned bilingual clauses.\n\n2. Ancient-Modern Chinese Dataset (AMCD) - This is a dataset of 4K aligned bilingual clauses from the AMCC.\n\n3. Augmented Parallel Corpus (APC", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The tweets in the OLID dataset are in English.\n\nQuestion: What is the annotation model used in the OLID dataset?\n\nAnswer: The annotation model used in the OLID dataset is a hierarchical three-level annotation model that encompasses:\n\n1. Using this annotation model, we create a new large publicly available dataset of English tweets.\n\n2. The key contributions of this paper are as follows:\n\na. Related Work: We expand on previous work and discuss the need for a typology that differentiates between whether the (abusive)", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The article mentions that the compound PCFG was trained on the PTB dataset, which is a Chinese corpus. The neural PCFG was also trained on the PTB dataset. The compound PCFG was also evaluated on the INLINEFORM0 dataset, which is a Chinese corpus.\n\nQuestion: what is the difference between the compound PCFG and the neural PCFG?\n\nAnswer: The compound PCFG is a probabilistic model that modulates rule probabilities with per-sentence continuous latent vectors. The neural PCFG is a neural network that directly optim", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has 5 layers:\n\n1. User embedding layer: This layer learns user embeddings for all users with minimum active degree, i.e., one post or one like.\n\n2. Topic information layer: This layer obtains topic information from the topic model or the pre-defined labels.\n\n3. Comment information layer: This layer obtains comment information from the comments.\n\n4. User embedding layer: This layer learns user embeddings for all users with minimum active degree, i.e., one post or one like.\n\n5. Topic information layer", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The paper uses the Flickr dataset, which contains geotagged photos from Flickr users. The dataset contains 70 million photos with geotags from 2015. The paper also uses the CORINE land cover dataset, which contains information about land cover types in Europe. The paper also uses the ScenicOrNot dataset, which contains people's subjective opinions of landscape beauty in Britain.\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is the proposed method to learn geographic location embeddings using Flickr tags", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper mentions that the experiments were conducted on two datasets:\n\n1. NUBes-PHI: a corpus of real medical reports manually annotated with sensitive information.\n\n2. MEDDOCAN: a dataset of clinical reports shared by the MEDDOCAN 2019 shared task.\n\nThe paper does not provide details on the specific clinical datasets used in the experiments.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "The article mentions that they used traditional linguistic features like word count, word frequency, and word length. They also used features derived from saliency graphs, which represent the reader's attention characteristics.\n\nQuestion: What new features did they introduce?\n\nAnswer: The article mentions that they introduced new features based on eye movement patterns, including simple gaze-based features and complex gaze-based features derived from a graph structure.\n\nQuestion: How did they improve the success rate of the sarcasm detector?\n\nAnswer: They improved the success rate of the sarcasm detector by 3.7", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The article mentions several metrics used to evaluate the performance of LiLi:\n\n1. Coverage: This metric measures the fraction of test instances for which LiLi was able to formulate a strategy that led to a winning prediction. The higher the coverage, the better LiLi is at formulating strategies.\n\n2. Avg. MCC and avg. +ve F1 score: These metrics measure the overall predictive performance of LiLi. The higher the values, the better LiLi is at predicting the labels.\n\n3. INLINEFORM15 and INLINEFORM18: These metrics measure", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "No, they do not employ their indexing-based method to create a sample of a QA dataset. The article states that they use the WikiQA dataset to create the sample of a QA dataset.\n\nQuestion: What is the size of the WikiQA dataset?\n\nAnswer: The WikiQA dataset has 15K+ questions and 536 Wikipedia articles.\n\nQuestion: What is the size of the SelQA dataset?\n\nAnswer: The SelQA dataset has 8K+ questions and 16K+ Wikipedia articles.\n\nQuestion: What is", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "The targets of the stance detection system are two popular sports clubs in Turkey: Galatasaray and Fenerbahçe. The corresponding annotations are made publicly available in the article.\n\nQuestion: What is the purpose of the stance detection system?\n\nAnswer: The purpose of the stance detection system is to detect stance in tweets about the two sports clubs. The system aims to identify whether a tweet expresses a positive, negative, or neutral stance towards the target sports club.\n\nQuestion: How is stance detected in the system?\n\nAnswer: The system uses a", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The article describes the following experiments:\n\n1. A dataset is built using tweets containing ironic sentences.\n\n2. A pre-training process is implemented using reinforcement learning to train the model.\n\n3. The model is implemented using two classifiers for non-ironic and ironic sentences.\n\n4. The model is evaluated using automatic and human evaluations.\n\n5. The model is compared with other generative models.\n\n6. The effectiveness of the proposed rewards is analyzed.\n\n7. The model is evaluated on additional experiments to transform ironic sentences to non", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention that uses a gaussian mask to capture localness and directional information of self-attention. The gaussian mask is a diagonal matrix with a standard deviation of 1, which means that the self-attention is only activated when the input is within a certain distance from the center. This allows the self-attention to focus on different words in a sentence, rather than just capturing the global information.\n\nThe gaussian mask is applied to the query, key, and value matrices in the self-attention. The query", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article mentions that they used Twitter data for their experiments. They also mention that they used Facebook data for the causality detection task.\n\nQuestion: What is the purpose of the causality detection task?\n\nAnswer: The purpose of the causality detection task is to identify whether a message contains a causal explanation. The authors mention that they are interested in identifying causal explanations in social media because they can be used to identify the causes of social phenomena.\n\nQuestion: What is the purpose of the causal explanation identification task?\n\nAnswer: The purpose of the causal explanation identification task is to identify", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The network's baseline features are the features extracted from the pre-trained models. These features are:\n\n1. Sentiment features: These are features extracted from the pre-trained sentiment model. They include features like polarity, subjectivity, and valence.\n\n2. Emotion features: These are features extracted from the pre-trained emotion model. They include features like happiness, sadness, anger, fear, disgust, surprise, and anticipation.\n\n3. Personality features: These are features extracted from the pre-trained personality model. They include features like", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks were:\n\n1. The number of clusters used in the feature extraction. This was varied from 250 to 1000.\n\n2. The number of clusters used in the clustering step. This was varied from 2 to 10.\n\n3. The number of clusters used in the fine-grained sentiment quantification task. This was varied from 2 to 10.\n\n4. The number of clusters used in the fine-grained sentiment classification task. This was varied from 2 to 10", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The system achieved the following scores:\n\nEI-Reg:\n\n- Development set: 0.716\n- Test set: 0.718\n\nEI-Oc:\n\n- Development set: 0.716\n- Test set: 0.718\n\nEI-Oc:\n\n- Development set: 0.716\n- Test set: 0.718\n\nEI-Reg:\n\n- Development set: 0.716\n- Test set: 0.718", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is 53 case reports with 8.1 million tokens.\n\nQuestion: What are the five entity types annotated in the corpus?\n\nAnswer: The five entity types annotated in the corpus are cases, conditions, factors, findings and modifiers.\n\nQuestion: What is the annotation process for the corpus?\n\nAnswer: The annotation process for the corpus involves medical experts annotating the case reports using WebAnno. The annotations include entity spans, entity labels, entity relationships and entity modifiers.\n\nQuestion: What are the annotation guidelines for", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert a cloze-style question to a naturally-looking question. Cloze-style questions are constructed by replacing words in a passage with blanks, and then filling in the blanks with candidate words. This can be done in a way that makes the question look natural and grammatical. For example, a cloze-style question might be \"What is the name of the company that makes the iPhone?\" The answer is \"Apple\". The question can be converted to a natural-looking question by replacing \"Apple\" with \"the company that makes the iPhone\". The question then becomes \"What", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "The paper considers the problem of how to leverage prior knowledge robustly in learning models for NLP tasks. They propose three regularization terms on top of generalized expectation criteria to address this problem. The regularization terms include incorporating neutral features, maximizing entropy, and using KL divergence. They evaluate their approaches on several commonly used text classification datasets.\n\nQuestion: What are the limitations of their approach?\n\nAnswer: The limitations of their approach include:\n\n1. The regularization terms require computing the true class distribution, which may not be available in practice.\n\n2. The KL divergence term", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The authors compare their model to the BERT-QA model of Li and Roth BIBREF6 , which is a state-of-the-art QA model that uses pretrained word embeddings BIBREF40 to generate question classification labels. They also compare to the QC model of Van-tu et al. BIBREF24 , which uses a heuristic to determine the termination depth of hypernym expansion.\n\nQuestion: What is the purpose of the QC model?\n\nAnswer: The QC model is designed to generate question classification labels for the BERT-QA", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The article states that the training sets for the new versions of ELMo are larger than the previous ones. The new versions use 20 million tokens instead of 15 million tokens. This is a 33% increase in the size of the training set.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus.\n\nQuestion: What is the size of the POS tags?\n\nAnswer: The POS tags have a size of 30.\n\nQuestion: What is the size of the word embeddings?\n\nAnswer: The word embeddings have a size of 300.\n\nQuestion: What is the size of the character embeddings?\n\nAnswer: The character embeddings have a size of 30.\n\nQuestion", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "The authors compare their proposed s2sL model to the following models/frameworks:\n\n1. MLP: A multi-layer perceptron (MLP) is considered as the baseline model for the s2sL approach.\n\n2. Eusboost: The authors also introduce a new method called Eusboost to handle imbalanced data.\n\n3. MWMOTE: The authors also introduce a new method called MWMOTE to handle imbalanced data.\n\n4. MFCC: The authors use Mel-frequency cepstral coefficients (MFCC", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, their NER model learns NER from both text and images. The paper states that their proposed MNER model \"combines the word and character embeddings at each decoding step\" and \"uses the modality attention module to the Bi-LSTM/CRF to produce a label for each token maximizing the following objective.\" This suggests that the model is learning NER from both text and images.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes, the article evaluates only on English datasets. The authors mention that their approach is \"specially designed for English\" and that they \"do not tune the hyperparameter for gold POS tags following the original experimental setup for DMV\".", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "The highest MRR score achieved by the UNCC_QA1 system was 0.2862 in the final test batch set. The UNCC_QA2 system achieved a higher MRR score of 0.2922 in the final test batch set.\n\nQuestion: What was the lowest F-Measure score achieved by the UNCC_QA1 system?\n\nAnswer: The lowest F-Measure score achieved by the UNCC_QA1 system was 0.0786 in the second test batch set. The UNCC_QA2 system achieved a", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The article evaluates on the Wall Street Journal (WSJ) corpus. They train on sections 02-21 of WSJ and evaluate on section 23. They also remove punctuations and train on sentences of length INLINEFORM0.\n\nQuestion: What is the difference between the Gaussian HMM and the Gaussian DMV-structured syntax model?\n\nAnswer: The Gaussian HMM is a Markov-structured syntax model that uses Gaussian distributions to model latent POS tags. The Gaussian DMV-structured syntax model is a Markov-structured syntax model that uses Gaussian", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers. The survey showed that 87.5% of the NLP related jobs belong to a few common tasks, including sentence classification, text matching, sequence labeling, MRC, etc. It also showed that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on. Based on the survey results, the authors conclude that many engineers face the challenge of choosing from multiple frameworks,", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "The article states that they achieve state-of-the-art results on the SimpleQuestions and WebQSP benchmarks. They also achieve comparable results on the KBQA end task.\n\nQuestion: What is the difference between the proposed HR-BiLSTM model and the BiLSTM model with shortcut connections?\n\nAnswer: The HR-BiLSTM model has hierarchical matching between questions and KB relations. It uses a BiLSTM relation detector to detect KB relations and then uses a BiLSTM entity re-ranking step to re-rank", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
