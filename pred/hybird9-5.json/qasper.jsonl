{"pred": "The ground truth for fake news is established through manual annotation by an expert. The expert annotated a subset of the tweets in the dataset as containing fake news or not containing fake news. This annotated dataset is publicly available and can be freely reused.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is a pooling strategy for aggregating frame-level features into a single utterance level feature. It was originally proposed for face recognition and was later used for language identification in this paper.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms the state of the art results by 68.8% to 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes using context tweets as additional features to improve the baseline models. Context tweets provide information about the context of the abusive tweet, which helps human annotators and computers better understand the abusive tweet. The article argues that context tweets can also help improve the baseline models.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at a subset of Facebook pages for training data. They chose pages that performed well on the development set and pages that had a distribution of emotions that was similar to the development set. They also grouped pages into sets based on performance and emotion distribution.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains hashtags in English, Spanish, French, German, Italian, Portuguese, and Chinese. The SemEval dataset contains tweets in English, Spanish, French, German, Italian, Portuguese, and Chinese.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The authors propose a new evaluation protocol and baseline method for the concept-map-based MDS task. They evaluate the performance of their model on the new corpus they created and provide baseline results. They also provide a detailed evaluation of the quality of the annotations created by their crowdsourcing scheme.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The authors evaluate their model on three summarization datasets: CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach of learning multi-sense word embeddings using KL divergence based energy function and asymmetric KL divergence approximation is shown to outperform other approaches on benchmark word similarity and entailment datasets. The proposed approach captures the best of both worlds by modeling both word similarity and entailment.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by selecting the best performing model from a pool of models at each step and then adding it to the ensemble. The algorithm starts with the best performing model and then selects the best performing model that has not been previously tried. If that model improves the ensemble's validation performance, it is added to the ensemble. The algorithm continues this process until all models have been tried and evaluated. The final ensemble is formed by averaging the predictions from the constituent models.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets are from two different sources. Friends is a speech-based dialogue dataset and EmotionPush is a chat-based dialogue dataset.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "The paper focuses on English.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "The IMDb dataset of movie reviews is used for sentiment analysis in this study.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an F1 score of 0.87 on the DL-PS dataset and 0.89 on the EC-MT and EC-UQ datasets.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "No", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The article mentions that the authors created a dataset of 1244 dialogues for training the intent classifier and a dataset of 4150 dialogues for testing the intent classifier. The authors also mention that they used a dataset of 124 dialogues to train the action classifier.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The best performing stock sector was the Healthcare sector, with a mean absolute error of 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared their proposed SMT model with various NMT models, including RNN-based NMT, Transformer-based NMT, and the Transformer model BIBREF4.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are:\n\n1. Incorporating neutral features, which are common words that are not indicative of any class. This is the simplest regularization term and does not require any modification of the GE-FL algorithm. \n\n2. Maximum entropy regularization, which aims to maximize the entropy of the class distribution. This regularization term does not require any extra knowledge but assumes that the categories are uniformly distributed.\n\n3. KL divergence regularization, which aims to minimize the KL divergence between the predicted class distribution and the reference class distribution. This", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with transformed word embedding, CNN and RCNN models, and UTCNN without user information.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved by 1.5-2.5% on the fine-grained sentiment classification task compared to the baseline systems.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The authors argue that their adaptively sparse transformer model, which uses sparse attention with adaptive alpha values, improves interpretability compared to softmax transformers. They provide evidence from their analysis of attention head behavior and qualitative examples of how the adaptive alpha values allow the model to learn to specialize attention heads more effectively. They also show that the learned attention heads tend to be more sparse and have higher confidence in their predictions, which improves interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline model used in the experiment was a Transformer model trained on the BIBREF11 dataset. The model was trained on 6 million instances and used a batch size of 4.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The authors evaluate their models on two cross-lingual zero-shot tasks: natural language inference (XNLI) and dependency parsing. They report accuracy and LAS (labeled attachment scores) for these tasks.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on the MT task during pre-training. It is trained to predict the target sentence given the source sentence.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The article mentions that stylistic features like \"unusual word order, ambiguity, syntactic complexity, and incongruity\" are obtained from the text. These features are used to distinguish sarcasm from other forms of semantic incongruity.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM encoder.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines in the article were:\n\n- Jasper5x3, a 5x3 version of the Jasper architecture with 2048 filters and 1024 units. \n\n- Jasper10x4, a 10x4 version of the Jasper architecture with 2048 filters and 2048 units.\n\n- Jasper10x3, a 10x3 version of the Jasper architecture with 2048 filters and 1024 units.\n\n- Jasper5x3-no-", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "They look at over 20,000 blog users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1, BLEU-4, ROUGE-L, and human evaluation are used for evaluation. BLEU-1 and BLEU-4 are used to measure the similarity between generated recipes and gold recipes in terms of n-gram matching. ROUGE-L measures the similarity between generated recipes and gold recipes in terms of recipe-level coherence. Human evaluation is used to measure the preference of human evaluators for generated recipes over baseline recipes.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create three labels:\n\n1. \"Answerable\": This label is given to the dialogue if the answer span contains a symptom mentioned in the query. \n\n2. \"No Answer\": This label is given to the dialogue if the answer span does not contain a symptom mentioned in the query.\n\n3. \"Unanswerable\": This label is given to the dialogue if the answer span contains a symptom mentioned in the query but the answer span is not mentioned in the query.\n\nSo in summary, they create three labels: \"Answerable\", \"No Answer\", and \"Unanswer", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The article does not provide a specific amount of data needed to train the task-specific encoder. The authors mention that the task-specific encoder was trained on a large corpus of biomedical text, but do not specify the exact size of the corpus.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The authors evaluate their model on four machine translation tasks: IWSLT, WMT14, WMT16, and WMT17. They report BLEU scores on these datasets for their models and baselines.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is 1.3% over the baseline fastText embeddings.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "The authors have backgrounds in computational linguistics, social sciences, and humanities. They have experience in analyzing text as social and cultural data. They have worked on projects that involve text mining, text analysis, and computational social science. They have also worked on projects that involve qualitative and quantitative analyses of text.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages (zul, xho, nbl, ssw), Afrikaans (afr) and English (eng) are similar to each other and harder to distinguish. The same is true of the Sotho languages (nso, sot, tsn), as well as tshiVenda (ven) and Xitsonga (tso). The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared their deep LSTM model with a 2-layer LSTM model trained with sMBR and a 2-layer LSTM model trained with CE loss. They also compared their deep LSTM model with a 2-layer LSTM model trained with sMBR and a 2-layer LSTM model trained with CE loss.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The data set consists of 29,794 articles from Wikipedia and 1,000 articles from arXiv. The Wikipedia data set has 5,000 articles from each quality class, and the arXiv data set has 1,000 articles from each quality class.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by a group of 50 native Tamil speakers who were well-versed in both English and Tamil languages. They were asked to evaluate the translations of the RNNMorph model and the RNNSearch model on a scale of 1 to 5 for adequacy, fluency and relative ranking. The translations were then ranked between each other and the judges were asked to estimate the adequacy, fluency and relative ranking of the translations. The judges were also asked to provide qualitative feedback on the translations.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "No, they test their framework on an under-resourced language pair, German-to-French. They do not test on commonly used language pairs.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by having human participants type a sentence and then compare the generated keywords to the actual sentence. The human participants are asked to determine if the generated keywords are semantically equivalent to the actual sentence. The model's performance is measured by the percentage of generated keywords that are semantically equivalent to the actual sentence.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The article mentions that the performance of the classifiers is evaluated using the following metrics:\n\n- Precision: the proportion of correctly classified sentences \n- Recall: the proportion of all sentences that are correctly classified\n- F-measure: the harmonic mean of precision and recall\n\n- ROUGE unigram score: a measure of the similarity between two summaries\n\n- Human evaluations: the performance of the summarization algorithm is compared to manual summaries by human evaluators\n\nSo in summary, the article looks at precision, recall, F-measure, ROUGE unigram", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain where the labeled data is available, and the target domain is the domain where the unlabeled data is available. The source domain contains labeled examples and the target domain contains unlabeled examples. The goal is to learn a classifier that can generalize to the target domain.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "The article compares their Pyramidal Recurrent Unit (PRU) model with several previous RNN models, including:\n\n- LSTM (Long Short-Term Memory) \n- GRU (Gated Recurrent Unit)\n- RNN (Recurrent Neural Network)\n- LSTM-GRU (LSTM with Gated Recurrent Unit)\n- LSTM-LSTM (LSTM with LSTM)\n- LSTM-LSTM-GRU (LSTM with LSTM and Gated Recurrent Unit)\n\nSo in", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes several commonly used neural network modules, such as word/character embedding, CNN/LSTM, CRF, BiLSTM, Attention, etc. These modules are categorized into groups according to their functions and encapsulated into standard and reusable blocks with a consistent interface.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 .", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines in the paper were:\n\n- BERT (BIBREF12)\n- XLNet (BIBREF21)\n- RoBERTa (BIBREF22)\n\nThe paper compared these three transformer-based architectures on the speculation detection and scope resolution tasks.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "The languages they use in their experiment are English, Spanish, Finnish, and German. They use these languages for training and testing their models.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a post from its latent representation. They formulate their training objective to maximize cross-entropy loss at the task of predicting hashtags for a post from its latent representation.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "No, they do not use pretrained embeddings. They use Glove embeddings as the initial embeddings for the words in the vocabulary.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyResponse was evaluated against a baseline system called \"PolyResponse-Baseline\" that uses a simple ranking function based on the cosine similarity of the text representations. The baseline system was found to be significantly worse at retrieving relevant responses, with an average recall of 0.3 compared to 0.7 for the PolyResponse system.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) lexicon, which groups words into categories based on their meaning. They then compile distributions for the entire category and generate maps for these word categories. For example, they generated maps for the categories \"Positive Feelings\" and \"Money\".", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components in user-generated text. The article discusses how Toulmin's model of argumentation is used as a basis for identifying argument components in user-generated text. The article argues that Toulmin's model is useful for identifying argument components in short texts like comments and forum posts.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "PARENT aligns n-grams of length 1 to 4 with the table and n-grams of length 1 to 4 with the reference.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets, from a publicly available resource.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The article covers 12 monolingual and 666 cross-lingual languages. The monolingual languages are: English, French, Spanish, German, Italian, Russian, Chinese, Japanese, Korean, Portuguese, Arabic, Swedish, Dutch, Danish, Finnish, Norwegian, Turkish, Welsh, Hungarian, Romanian, Polish, and Estonian. The cross-lingual languages are: English, Spanish, German, French, Italian, Russian, Chinese, Japanese, Korean, Portuguese, Arabic, Swedish, Danish, Finnish, Norwegian, Turkish, Welsh, Hungarian", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "The model is applied to the Conversations Gone Awry dataset and the ChangeMyView dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were based on rule-based and statistical models. The authors did not mention any deep learning models being used.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated through various sanity checks and metrics. The authors perform sentence-level BLEU checks, manually inspect examples with low BLEU scores, measure perplexity, compute the ratio of English characters, and use VizSeq to calculate similarity scores. They also check the overlap between CoVo transcripts and Tatoeba sentences to ensure they are disjoint.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use a dual RNN architecture, where one RNN encodes the audio sequence and the other RNN encodes the text sequence. The output of each RNN is then concatenated and passed through a fully connected neural network layer to generate a combined representation of the audio and text sequences. This combined representation is then used to predict the emotion class.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The authors report that their model, which includes synthetic data from simplified corpora, improves the text simplification performance of the baseline NMT model by 2.11 BLEU, 1.7 FKGL and 1.07 SARI, compared to the baseline NMF model. The human evaluation results also show that the synthetic data helps produce simpler output.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "The article does not provide information on how many humans evaluated the results.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets going viral are those that were retweeted more than 1000 times by the 8th of November 2016.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "The CNN model performs best by itself, as shown in Table TABREF10 where the CNN model outperforms the other models (Logistic Regression, BERT and LSTM-CRF) on the SLC task.", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data is collected from crowdsourcing. Respondents were recruited through Amazon Mechanical Turk and were paid to record speech samples. The data was collected over a period of 1.5 years.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The paper uses Logistic Regression and a neural network model based on the Transformer architecture. The neural network model is trained on a dataset of question-answer pairs and is used to classify questions as entailed or not. The Logistic Regression model is used to rank the retrieved questions based on their classification scores.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset, which contains 22,223 spammers and 19,276 legitimate users. The quality of the dataset is high as it was created and deployed by researchers to attract spammers and collect data. The dataset was collected over 7 months and contains tweet content from both legitimate users and spammers.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder-decoder architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No, they report results on three datasets in English, Chinese and Korean.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble of LSTM-CRF and BERT, which is ranked 3rd in the FLC task and 4th in the SLC task. The performance of this ensemble is not explicitly mentioned in the article.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline for the experiments was a uni-directional PBSMT model trained on the in-domain parallel data for each language pair. The BLEU scores of this baseline model were reported in Table TABREF22.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The highest recall score for the BioBERT model was 0.70333, which was achieved in the second test batch.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores integrating semantic similarity measures into second-order co-occurrence vectors and integrating semantic similarity scores derived from a taxonomy into the second-order vectors. It also explores integrating word embeddings derived from structured corpora such as literature and dictionary entries into a second-order vector or using them on their own.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use a pre-ordering system to match the words before reordering them. The pre-ordering system contains two reordering systems: (1) generic rules that apply to all Indian languages and (2) Hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering. The Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The article states that seven experts with legal training were recruited to construct answers to questions posed by crowdworkers. These experts identified relevant evidence within the privacy policy, as well as provide meta-annotation on the question's relevance, subjectivity, OPP-115 category and how likely any privacy policy is to contain the answer to the question asked.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The painting embedding model is a CNN-RNN actor-critic model. The language style transfer model is a sequence-to-sequence model with global attention.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better on long documents, as shown in the experiments. The transformer layer is able to capture long-range dependencies between words in a sequence, which is important for tasks involving long texts.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "No", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The article discusses cyberbullying detection for three topics: personal attack, racism, and sexism. The authors experimented with four DNN models for each topic.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The new context representation is obtained by splitting the sentence into three parts: left context, middle context and right context. The middle context is then used as input for the CNN. The left and right contexts are used as input for the RNN. The middle context is then combined with the left and right contexts to form the extended middle context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The article does not provide information on the number of different types of entities that exist in the dataset. The number of entities is only given in terms of total entities, PER, LOC, ORG and MISC.", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The article states that \"Expert annotations are preferable whenever its collection is practical. But in real-world settings, a combination of expert and lay annotations is better than using lay data alone.\" This suggests that expert annotations are of higher quality than lay annotations, and that a combination of expert and lay annotations is better than using lay data alone.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The article states that women represent 33.16% of the speakers, but they account for only 22.57% of the total speech time. This indicates a significant imbalance in the analyzed corpora.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "This approach achieves state of the art results on the English-German dataset, as compared to BIBREF30 .", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The paper compares the proposed model to several strong baseline models, including:\n\n- BIBREF18: A Transformer model with unigram features and greedy decoding\n- BIBREF20: A Transformer model with $n$-gram features and greedy decoding\n- BIBREF21: A Transformer model with unigram features and greedy decoding, but with pre-trained embeddings\n- BIBREF22: A Transformer model with $n$-gram features and greedy decoding, but with pre-trained embeddings\n\n", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic regression and multilayer perceptron are used as the target classifiers for the human-AI loop approach.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, CogComp-NLP, and Stanford NLP NER.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The experiments are performed on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. The data is split into dev and test sets.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The article mentions several existing approaches for learning geographic location embeddings. Some of the approaches mentioned are:\n\n- Bag-of-words representations derived from Flickr tags, as in BIBREF7 \n- GloVe word embedding model, as in BIBREF9\n- Lexicons that impose constraints on the embeddings to capture semantic types, as in BIBREF13\n- Models that use the assumption that salient properties of a given domain can be modelled as directions in vector space embeddings, as in BIBREF10 \n\nSo in summary, the", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention in the contextual encoding layer and the memory generation layer. The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2 .", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used the CSAT dataset for customer satisfaction prediction, the 20 newsgroups dataset for topic identification, and the Fisher dataset for topic identification.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "The IMDb movie review dataset is used for sentiment classification experiments. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No. The tasks were created specifically for this study to evaluate the BERT model's ability to capture syntax-sensitive structures.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No, the datasets for sentiment analysis are not balanced. The article states that \"The dataset of 1,000 tweets contains more than twice as many tweets about Trump than about the other candidates.\" This imbalance in the dataset makes it difficult for existing NLP systems to accurately perform sentiment analysis of political tweets.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a requirement that the neural projector used to map latent embeddings to observed embeddings must be invertible. This means that the Jacobian matrix of the neural projector must be non-zero and differentiable. The invertibility condition is used to ensure that the neural projector can be inverted to compute the marginal likelihood of the latent variables.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema is a set of categories that describe the linguistic features and reasoning capabilities probed by the gold standards. The categories include:\n\n- Required Reasoning: Describes the type of reasoning required to answer the question, such as temporal succession, spatial reasoning, causal reasoning, etc.\n\n- Lexical Features: Describes the linguistic features that are present in the context and question, such as synonyms, paraphrases, redundancy, etc.\n\n- Factual Correctness: Describes issues with the factual correctness of the gold standards, such as", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The WikiSmall dataset has 89,042 sentence pairs and 11.6M words. The WikiLarge dataset has 296,402 sentence pairs and 11.6M words.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The article mentions several baselines for comparison, including:\n\n- Vanilla ST baseline: The vanilla ST model trained from scratch on the ST-TED corpus.\n\n- Pre-training baselines: \n\t- Encoder pre-training: The ST encoder is initialized from an ASR model.\n\t- Decoder pre-training: The ST decoder is initialized from an MT model.\n\t- Encoder-decoder pre-training: Both the encoder and decoder are pre-trained.\n\n- Multi-task baselines: \n\t- Many-", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "The paper studies English and Russian. The authors note that they are working with a corpus of Russian news articles in addition to English news articles.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The article mentions that a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiments.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: Yes\n\nQuestion: Do the answered questions measure for the relevance of the answer?\n\nAnswer: Yes\n\nQuestion: Do the answered questions measure for the readability of the answer?\n\nAnswer: Yes\n\nQuestion: Do the answered questions measure for the engagement of the answer?\n\nAnswer: Yes\n\nQuestion: Do the answered questions measure for the expertise of the answer?\n\nAnswer: No\n\nQuestion: Do the answered questions measure for the timeliness of the answer?\n", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 and Edinburgh embeddings BIBREF14 were used.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The authors report that their personalized models outperform baseline models in BPE perplexity, BLEU-1/4, and ROUGE-L, and generate more diverse recipes. However, the authors also note that BLEU and ROUGE are not the most appropriate metrics for generation quality.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence. The sentiment reward is the standardized sentiment score of the input sentence and the output sentence. The harmonic mean of these two rewards is used as the combined reward to maximize the expected reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate a limitation of their model by showing an example where the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared their model to the following existing models:\n\n- BIBREF10: Affective Text dataset, which they used for training and testing their models.\n\n- BIBREF11: ISEAR dataset, which they used for training and testing their models.\n\n- BIBREF12: Affective Text dataset, which they used for training and testing their models.\n\n- BIBREF13: Affective Text dataset, which they used for training and testing their models.\n\n- BIBREF14: Affective Text dataset, which they used for training", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The article reports that there were no statistically significant differences in the distribution of retweets, favourites, hashtags, mentions, or media elements between tweets containing fake news and tweets not containing them.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 . The hashtags are extracted from the tweets in the dataset and then segmented into words. The segmented hashtags are then used as the input for the hashtag segmentation model.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The article does not provide information on the accents present in the corpus. The corpus is described as containing speech from \"at least a few thousand speakers\" and \"speakers from different regions and backgrounds\". However, the specific accents present are not mentioned.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the context of a text. It can model the word vectors of each class as word subspaces and efficiently compare them in terms of similarity by using canonical angles between the word subspaces.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The article does not specify which baseline model is used.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The Augmented LibriSpeech dataset is a large-scale speech recognition corpus with over 11,000 speakers and over 10 accents. It is created by augmenting the transcripts of the LibriSpeech corpus with English translations. The dataset is over 708 hours long and covers 11 languages into English.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "They used the SemEval-2016 \"Sentiment Analysis in Twitter\" dataset for the fine-grained sentiment classification task.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained BERT model, which is a large model with 110M parameters.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "No. The authors note that while they are able to generate large amounts of systematically controlled data at virtually no cost or need for manual annotation, it is much harder to validate the quality of such data at such a scale and such varying levels of complexity. Conversely, with benchmark QA datasets, it is much harder to perform the type of careful manipulations and cluster-based analyses they report here. While they assume that the expert knowledge they employ is generally correct, they know that such resources are fallible and error-prone. Initial crowd-sourcing experiments that look at validating samples of their data", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "No, the images are from a general domain of visual scenes. The authors state that the ShapeWorldICE datasets are generated from a large corpus of images from the internet.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The authors' best model (B-M) achieved an average f-score of 0.368 on the development set, which is the highest score among the models they tested.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is a simple one-hot scheme where a word is tagged as a pun if it is the first word in a context that contains a pun. If a word is not the first word in a context, it is tagged as not a pun. The tagging scheme is designed to capture the property that a pun tends to appear at the second half of a sentence.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The authors define robustness as the ability of a model to generalize to new data that is not seen during training. They argue that a model is robust if it performs well on unseen data that is similar to the training data. They evaluate their proposed methods on datasets with unbalanced class distributions and unbalanced labeled features to test their robustness.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "The article evaluates SBERT against InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method improves F1 score for the NER task on both English and Chinese datasets by a large margin, outperforming the best baseline BERT-MRC by +1.86 on CoNLL2003 and +2.36 on OntoNotes4.0 for Chinese datasets. The proposed DSC loss performs robustly on all four NER datasets.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: Quora duplicate question pair detection and ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "The authors compared their model against several baselines, including:\n\n- Syntactic tree-LSTM: A tree-LSTM model that directly uses the syntactic tree structure as input.\n\n- Feed-forward neural network: A feed-forward neural network that directly uses the input words as input.\n\n- Tree-LSTM: A tree-LSTM model that directly uses the syntactic tree structure as input.\n\n- Feed-forward neural network: A feed-forward neural network that directly uses the input words as input.\n\n- Syntactic tree-", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the KB relation detection model, HR-BiLSTM, which performs hierarchical matching between questions and KB relations. The model outperforms previous methods on KB relation detection tasks and allows the KBQA system to achieve state-of-the-art results.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are a simple encoder-decoder model with ingredient attention and a name-based Nearest-Neighbor model. The encoder-decoder model is a two-layer GRU with hidden state $h_t$ conditioned on previous hidden state $h_{t-1}$ and input token $w_{r, t}$ from the original recipe text. The name-based Nearest-Neighbor model initializes the decoder hidden state with the name of the most similar recipe from the training set.", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The article discusses several methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "The article explores the Winograd Schema Challenge, which is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun. The challenge is administered by commonsensereasoning.org and sponsored by Nuance Inc. It was offered for the first time at IJCAI-2016 (Morgenstern, Davis, and Ortiz, in preparation); the organizers plan to continue to offer it roughly once a year", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n- Stacked LSTMs \n- Cell-aware Stacked LSTMs (CAS-LSTMs)\n- Bi-CAS-LSTMs (bidirectional CAS-LSTMs)\n- MDRNNs (multidimensional RNNs)\n- MDLSTMs (multidimensional LSTMs)\n\nSo in summary, they experimented with various LSTM-based models, including stacked LSTMs, CAS-LSTMs, bidirection", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they report results on English and German data. They evaluate their method on both English and German embeddings.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms provided by the Sumy package. They set the number of sentences parameter to the ceiling of K/3 for each algorithm. Table TABREF51 shows the average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "BIBREF7 proposed models with discourse features to enable better prediction over unseen MOOCs. However, their model required a hyperparameter for the number of latent states. It is likely that their empirically reported setting will not generalise due to their weak evaluation.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The answer is \"No master node\". The article states that removing the master node deteriorates performance across all datasets, showing the value of having such a node. The master node is connected to all other nodes, which enables it to encode during message passing a summary of the document.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The article mentions that they experiment with 7 Indian languages: Hindi, Telugu, Malayalam, Bengali, Tamil, and Kannada.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not reported in the article. The article focuses on exploring the cross-lingual transfer learning ability of the model on RC tasks in the zero-shot setting.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model ALOHA outperforms the baselines by a significant margin, achieving an average Hits@1/20 score of 40.67% compared to the baseline's 34.33%. The difference is 6.34% on average, demonstrating the effectiveness of the proposed HLA-based character dialogue clustering approach.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article states that ARAML outperforms the baselines in terms of forward perplexity by 1.5-2.5% and Self-BLEU by 1.5-2.5% on COCO and 1.5-2.5% on WeiboDial. On WeiboDial, ARAML also performs significantly better than other baselines in terms of grammaticality and relevance.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the confusion matrices of the model and performing a manual inspection of a subset of the data. They find that the model misclassifies some tweets as hate speech when they contain words that are associated with femininity or contain subtle insults. They also find that the model misclassifies some tweets as offensive when they contain words that are associated with African American Vernacular English or are written within the United States of America. They argue that these misclassifications are due to biases in the data collection and annotation process", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "No, only the neural baselines were tested. The article states that \"We describe baselines on this task, including a human performance baseline.\" but does not provide any details about the other baselines tested.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively. The total dataset size is not mentioned in the article.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method of replacing cross-entropy loss with dice loss improves F1 score for paraphrase identification by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article does not specify which datasets are used. The authors mention that they have access to \"eye-tracking data, self-paced reading time, and ERP data\" but do not specify which datasets those are.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of visual stimuli that were designed to elicit event-related responses. The visual stimuli included a series of images of faces, objects, and scenes. The subjects were instructed to imagine themselves saying the words associated with the visual stimuli. The EEG data was recorded from the scalp of the subjects during the presentation of the visual stimuli and the imagined speech.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+RL-ROUGE, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The article reports the baseline accuracy of different learning models as well as their variants on the dataset. The models used are:\n\n- Naive Bayes\n- Logistic Regression\n- Support Vector Machine\n- Random Forests\n- Gradient Boosted Trees\n- Convolutional Neural Networks\n- Recurrent Neural Networks\n- HybridCNN\n- Bidirectional GRU networks with Latent Topic Clustering\n\nSo in summary, the article reports the baseline accuracy of various machine learning and neural network models on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The article does not specify the exact language model architectures used. However, it mentions that they use a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention and the uni-directional model contains INLINEFORM0 transformer blocks.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the training loss. The model optimizes the weights jointly with the model parameters during training. The weights are multiplied with a decaying factor $(1-p)$ to push down the weight of easy examples.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies, KG-A2C-chained and KG-A2C-Explore, are shown to outperform the baseline A2C and KG-A2C methods. KG-A2C-chained is significantly more sample efficient and converges faster, while KG-A2C-Explore takes longer to reach a similar reward but consistently makes it through the bottleneck. The knowledge graph cell representation appears to be a better indication of what a promising state is as opposed to just the textual observation.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a set of parameters that are learned from the training data. The parameters are learned using Bayesian inference, which involves sampling from a prior distribution over the model parameters and updating the parameters based on the likelihood of the observed data. The model parameters are then used to generate predictions for new data.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The article mentions that the transcriptions of the Mapudungun speech corpus include annotations for non-standard pronunciations, such as aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. These non-standard pronunciations are labeled as such in the transcriptions.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a neural network architecture that processes a sequence of words by treating each word as a sequence of characters. The architecture is agnostic to the order of the characters within the words.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The article explores 16 different languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "The NCEL approach is effective overall, as shown by the results on five different datasets. NCEL consistently outperforms baseline collective entity linking methods, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The key modules of NCEL, such as the attention mechanism and the global module, are also effective in improving the robustness and generalization ability of the model.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the corrected version of the same FCE training set on which the system is trained (450K tokens), and example sentences extracted from the English Vocabulary Profile (270K tokens).", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2/VA dataset. The i2b2/VA dataset is a collection of clinical notes from the VA hospital system. The notes were annotated with clinical entities and terms. The annotated notes were used as the training data for the BiLSTM-CRF model.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the decoder to learn to generate context vectors for the masked words, which helps the decoder learn to generate more contextually aware word representations. This helps the decoder generate more fluent and natural sequences.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "They do not specify which dataset they use. The article focuses on the survey of existing tweet representation models and their categorization based on the objective function.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used. TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a useful weighting scheme in information retrieval and text mining. TF-IDF signifies the importance of a term in a document within a corpus. It is important to note that a document here refers to a pathology report, a corpus refers to the collection of reports, and a term refers to a word.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated by researchers who review the tweets and determine whether they contain evidence of depression or not. If a tweet contains evidence of depression, it is further annotated with one or more depressive symptoms, for example, depressed mood. The annotators are trained to identify depression-related tweets.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The authors evaluated on the following eight NER tasks:\n\n- MEDIC-NER\n- MIMIC-NER\n- MIMIC-NER-v2\n- MIMIC-NER-v3\n- MIMIC-NER-v4\n- MIMIC-NER-v5\n- MIMIC-NER-v6\n- MIMIC-NER-v7\n- MIMIC-NER-v8\n\nSo in summary, they evaluated on the MIMIC-NER, MIMIC-NER-v2, MIMIC-NER-v3", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated into Spanish using the machine translation platform Apertium. The translations were then added to the original training data.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a content-based classifier in conjunction with two feature selection methods.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for this task was a very simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence. The performance of this baseline on the SLC task was shown in Tables TABREF33 and TABREF34.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The authors compare their model with a baseline model based on conditional random fields (CRF). They also compare their model with a baseline model that does not adopt joint learning.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. The authors refer to BIBREF2 to label different outlets as left-biased, right-biased, or neutral. This political bias is then used to train the classifier on a subset of news articles from each source.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from a large corpus of ancient Chinese texts collected from the internet. The texts are from various sources like ancient Chinese history records and articles written by celebrities. The texts are then segmented into paragraphs and clauses, and the clauses are aligned with their modern Chinese counterparts. The resulting dataset contains 1.24 million bilingual sentence pairs.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The tweets are in English.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The article mentions that the compound PCFG was trained on the PTB corpus in English and Chinese. The Chinese corpus used were the PTB-CN and the CN-PTB corpus.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has 10 layers.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The paper uses Flickr tags and structured environmental datasets to represent geographic locations. The Flickr tags are used to capture the textual information about locations, while the structured datasets provide numerical and categorical features. The datasets used include geographic locations, numerical features like temperature and precipitation, and categorical features like land cover types.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two clinical datasets for the experiments:\n\n- NUBes-PHI: A corpus of real medical reports manually annotated with sensitive information. This corpus is not publicly available.\n\n- MEDDOCAN: A dataset of clinical documents from the MEDDOCAN shared task competition. The authors note that the MEDDOCAN dataset is not publicly available.\n\nSo in summary, the paper uses the NUBes-PHI corpus and the MEDDOCAN shared task dataset for the experiments.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "The authors used unigram features, pragmatic features, stylistic patterns, and patterns related to situational disparity.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The article does not provide specific metrics to establish that LiLi makes chatbots more knowledgeable and better at learning and conversation. The article focuses on describing the LiLi approach and its effectiveness in solving the OKBC problem. The metrics used to evaluate LiLi are based on the performance of LiLi in predicting whether a query is in the knowledge base and formulating inference strategies. The article does not provide metrics to show that LiLi makes chatbots more knowledgeable or better at learning and conversation.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "No", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "The article mentions that the targets of the annotated stances are two popular sports clubs in Turkey, namely Galatasaray and Fenerbahçe.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The paper conducts experiments on the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences. The experiments are conducted on the large-scale dataset of ironic and non-ironic sentences built by the authors. The authors also conduct additional experiments on the transformation from ironic sentences to non-ironic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention that replaces the standard self-attention in the Transformer encoder. It works by generating the representation of sentences at once using queries, keys and values of input sequences. The key difference is that it uses a Gaussian mask to mask out the values of the keys and values of the queries, which allows the self-attention layer to generate the representation of sentences at once and contain the information of the sentence. The Gaussian mask is used to model the localness and directional information of the self-attention based encoder.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The authors considered social media text from Facebook, Twitter, and Reddit. They did not specify the exact sources or time periods for these social media platforms.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the pre-trained sentiment, emotion and personality models. These features are used as the input to the network's fully-connected layer. The features extracted from the pre-trained models are used as the input to the network's fully-connected layer.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The article does not specify which hyperparameters were varied in the experiments on the four tasks.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system are shown in Table TABREF18 . For the EI-Reg task, the best score is 0.716. For the EI-Oc task, the best score is 0.718. For the V-Reg task, the best score is 0.716. For the V-Oc task, the best score is 0.718.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "No, the article states that the cloze-style questions generated by the system are not naturally looking questions. They are generated based on a heuristic that assumes the first 20% of a document is the introduction and the remaining passages are the body. The cloze-style questions are constructed by replacing a placeholder in the answer bearing sentence with a passage from the body. The cloze-style questions are not paraphrased from the passage, which is why they are not naturally looking.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "The article considers text classification, sentiment analysis, and named entity recognition as NLP tasks.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The authors compare their model to several previous question classification methods, including:\n\n- Rule-based methods that make use of hand-crafted or semi-automated syntactic or semantic extraction rules to infer question classes. \n\n- Machine learning models that make use of pre-trained word embeddings BIBREF40 as feature extractors in a CNN model.\n\n- A BERT QA model that uses the same QA paradigm as the authors' model, where QA is modeled as a next-sentence prediction task that predicts the likelihood of a given multiple choice", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these new versions of ELMo are significantly larger than the previous ones. The new versions were trained on corpora of 20 million tokens, while the previous versions were trained on corpora of 10 million tokens. The larger training sets likely contributed to the improved performance of the new ELMo models.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 10000 sentences.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "The authors compare their proposed s2sL framework to the following models/frameworks:\n\n- MLP (modified to handle their data representation)\n- Eusboost [22]\n- MWMOTE [23]\n- Eusboost [22]\n- MWMOTE [23]\n\nSo in summary, the authors compare their s2sL framework to the following models: MLP, Eusboost, MWMOTE, Eusboost, and MWMOTE.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "No. The NER model learns NER from text only. The visual contexts are used to provide additional information to help disambiguate polysemous entities.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "The highest MRR score for the systems was 0.70333 for the UNCC_QA1 system trained on BioASQ data.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this claim through a survey they conducted among engineers. They found that many engineers reported that choosing from multiple frameworks, models and optimization techniques was a big overhead. The survey results showed that 40% of engineers reported that choosing from multiple frameworks was a big overhead, and 30% reported that choosing from multiple models was a big overhead.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "On the WebQSP and SimpleQuestions benchmarks, they achieve the state of the art results.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
