{
    "model": {
        "model_name": "meta-llama/Llama-2-7b-chat-hf",
        "model_dtype": "bf16",
        "model_method": "enc19",
        "model_structure": "llama",
        "save_ckp": "ckp/low1-ft5.pth",
        "load_ckp": "ckp/low1-pre.pth",
        "config": "config/low1.json",
        "device_map": {
            "encoder.retrieval_token": 0,
            "encoder.encoder.base_model.model.model.embed_tokens": 0,
            "encoder.encoder.base_model.model.model.layers.0": 0,
            "encoder.encoder.base_model.model.model.layers.1": 0,
            "encoder.encoder.base_model.model.model.layers.2": 0,
            "encoder.encoder.base_model.model.model.layers.3": 0,
            "encoder.encoder.base_model.model.model.layers.4": 0,
            "encoder.encoder.base_model.model.model.layers.5": 0,
            "encoder.encoder.base_model.model.model.layers.6": 0,
            "encoder.encoder.base_model.model.model.layers.7": 0,
            "encoder.encoder.base_model.model.model.layers.8": 1,
            "encoder.encoder.base_model.model.model.layers.9": 1,
            "encoder.encoder.base_model.model.model.layers.10": 1,
            "encoder.encoder.base_model.model.model.layers.11": 1,
            "encoder.encoder.base_model.model.model.layers.12": 1,
            "encoder.encoder.base_model.model.model.layers.13": 1,
            "encoder.encoder.base_model.model.model.layers.14": 1,
            "encoder.encoder.base_model.model.model.layers.15": 1,
            "encoder.encoder.base_model.model.model.layers.16": 2,
            "encoder.encoder.base_model.model.model.layers.17": 2,
            "encoder.encoder.base_model.model.model.layers.18": 2,
            "encoder.encoder.base_model.model.model.layers.19": 2,
            "encoder.encoder.base_model.model.model.layers.20": 2,
            "encoder.encoder.base_model.model.model.layers.21": 2,
            "encoder.encoder.base_model.model.model.layers.22": 2,
            "encoder.encoder.base_model.model.model.layers.23": 2,
            "encoder.encoder.base_model.model.model.layers.24": 3,
            "encoder.encoder.base_model.model.model.layers.25": 3,
            "encoder.encoder.base_model.model.model.layers.26": 3,
            "encoder.encoder.base_model.model.model.layers.27": 3,
            "encoder.encoder.base_model.model.model.layers.28": 3,
            "encoder.encoder.base_model.model.model.layers.29": 3,
            "encoder.encoder.base_model.model.model.layers.30": 3,
            "encoder.encoder.base_model.model.model.layers.31": 3,
            "encoder.encoder.base_model.model.model.norm": 3,
            "encoder.encoder.base_model.model.lm_head": 3,
            "decoder.repeat_token": 0,
            "decoder.decoder.base_model.model.model.embed_tokens": 0,
            "decoder.decoder.base_model.model.model.layers.0": 0,
            "decoder.decoder.base_model.model.model.layers.1": 0,
            "decoder.decoder.base_model.model.model.layers.2": 0,
            "decoder.decoder.base_model.model.model.layers.3": 0,
            "decoder.decoder.base_model.model.model.layers.4": 0,
            "decoder.decoder.base_model.model.model.layers.5": 0,
            "decoder.decoder.base_model.model.model.layers.6": 0,
            "decoder.decoder.base_model.model.model.layers.7": 0,
            "decoder.decoder.base_model.model.model.layers.8": 1,
            "decoder.decoder.base_model.model.model.layers.9": 1,
            "decoder.decoder.base_model.model.model.layers.10": 1,
            "decoder.decoder.base_model.model.model.layers.11": 1,
            "decoder.decoder.base_model.model.model.layers.12": 1,
            "decoder.decoder.base_model.model.model.layers.13": 1,
            "decoder.decoder.base_model.model.model.layers.14": 1,
            "decoder.decoder.base_model.model.model.layers.15": 1,
            "decoder.decoder.base_model.model.model.layers.16": 2,
            "decoder.decoder.base_model.model.model.layers.17": 2,
            "decoder.decoder.base_model.model.model.layers.18": 2,
            "decoder.decoder.base_model.model.model.layers.19": 2,
            "decoder.decoder.base_model.model.model.layers.20": 2,
            "decoder.decoder.base_model.model.model.layers.21": 2,
            "decoder.decoder.base_model.model.model.layers.22": 2,
            "decoder.decoder.base_model.model.model.layers.23": 2,
            "decoder.decoder.base_model.model.model.layers.24": 3,
            "decoder.decoder.base_model.model.model.layers.25": 3,
            "decoder.decoder.base_model.model.model.layers.26": 3,
            "decoder.decoder.base_model.model.model.layers.27": 3,
            "decoder.decoder.base_model.model.model.layers.28": 3,
            "decoder.decoder.base_model.model.model.layers.29": 3,
            "decoder.decoder.base_model.model.model.layers.30": 3,
            "decoder.decoder.base_model.model.model.layers.31": 3,
            "decoder.decoder.base_model.model.model.norm": 3,
            "decoder.decoder.base_model.model.lm_head": 3,
            "teacher": "cpu"
        }
    },

    "train": {
        "tbptt": 2,
        "train_iters": 1000,
        "max_lr": 1e-4,
        "warmup": 0.01,
        "corpus": [
            {
                "name": "redpajama book sample.train.128k",
                "partition": 0.025,
                "truncation": 25600
            },
            {
                "name": "redpajama arxiv sample.train.128k",
                "partition": 0.025,
                "truncation": 25600
            },
            {
                "name": "longalpaca.train.false",
                "partition": 0.7,
                "truncation": 8192
            },
            {
                "name": "longalpaca.train.false",
                "partition": 0.25,
                "truncation": 16384
            }
        ],
        "accum_grad": 1,
        "clip_grad": 1.0,

        "save": 1000,
        "eval": 1000,
        "tasks": [
            {
                "task_type": "perplexity",
                "task_name": "pg19.test.1m",
                "num_instance": 10,
                "truncation": 25600
            }
        ]
    }
}