{
    "chunk_size": 128,
    "enable_lora": false,
    "fix_layers": 0,
    "lora_kwargs": {
        "lora_rank": 128,
        "lora_alpha": 512,
        "lora_dropout": 0
    }
}