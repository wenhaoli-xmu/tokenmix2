{
    "chunk_size": 32,
    "enable_lora": true,
    "fix_layers": "[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]",
    "lora_kwargs": {
        "lora_rank": 128,
        "lora_alpha": 512,
        "lora_dropout": 0
    }
}